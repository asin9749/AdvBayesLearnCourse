<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="journal_id" content="11222"/>

    <meta name="dc.title" content="Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC"/>

    <meta name="dc.source" content="Statistics and Computing 2016 27:5"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2016-08-30"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Springer Science+Business Media New York"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan."/>

    <meta name="prism.issn" content="1573-1375"/>

    <meta name="prism.publicationName" content="Statistics and Computing"/>

    <meta name="prism.publicationDate" content="2016-08-30"/>

    <meta name="prism.volume" content="27"/>

    <meta name="prism.number" content="5"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1413"/>

    <meta name="prism.endingPage" content="1432"/>

    <meta name="prism.copyright" content="2016 Springer Science+Business Media New York"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s11222-016-9696-4"/>

    <meta name="prism.doi" content="doi:10.1007/s11222-016-9696-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s11222-016-9696-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s11222-016-9696-4"/>

    <meta name="citation_journal_title" content="Statistics and Computing"/>

    <meta name="citation_journal_abbrev" content="Stat Comput"/>

    <meta name="citation_publisher" content="Springer US"/>

    <meta name="citation_issn" content="1573-1375"/>

    <meta name="citation_title" content="Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC"/>

    <meta name="citation_volume" content="27"/>

    <meta name="citation_issue" content="5"/>

    <meta name="citation_publication_date" content="2017/09"/>

    <meta name="citation_online_date" content="2016/08/30"/>

    <meta name="citation_firstpage" content="1413"/>

    <meta name="citation_lastpage" content="1432"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s11222-016-9696-4"/>

    <meta name="DOI" content="10.1007/s11222-016-9696-4"/>

    <meta name="citation_doi" content="10.1007/s11222-016-9696-4"/>

    <meta name="description" content="Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction a"/>

    <meta name="dc.creator" content="Aki Vehtari"/>

    <meta name="dc.creator" content="Andrew Gelman"/>

    <meta name="dc.creator" content="Jonah Gabry"/>

    <meta name="dc.subject" content="Statistics and Computing/Statistics Programs"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Statistical Theory and Methods"/>

    <meta name="dc.subject" content="Probability and Statistics in Computer Science"/>

    <meta name="citation_reference" content="Akaike, H.: Information theory and an extension of the maximum likelihood principle. In: Petrov, B.N., Csaki, F. (eds.) Proceedings of the Second International Symposium on Information Theory, pp. 267&#8211;281. Akademiai Kiado, Budapest (1973)"/>

    <meta name="citation_reference" content="citation_journal_title=Int. J. Forecast.; citation_title=Predictive likelihood for Bayesian model selection and averaging; citation_author=T Ando, R Tsay; citation_volume=26; citation_publication_date=2010; citation_pages=744-763; citation_doi=10.1016/j.ijforecast.2009.08.001; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Stat. Surv.; citation_title=A survey of cross-validation procedures for model selection; citation_author=S Arolot, A Celisse; citation_volume=4; citation_publication_date=2010; citation_pages=40-79; citation_doi=10.1214/09-SS054; citation_id=CR3"/>

    <meta name="citation_reference" content="Bernardo, J.M., Smith A.F.M.: Bayesian Theory. Wiley, New York (1994)"/>

    <meta name="citation_reference" content="citation_journal_title=Biometrika; citation_title=A comparative study of ordinary cross-validation, 
                      
                      
                      
                    -fold cross-validation and the repeated learning-testing methods; citation_author=P Burman; citation_volume=76; citation_publication_date=1989; citation_pages=503-514; citation_doi=10.1093/biomet/76.3.503; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Electron. J. Stat.; citation_title=Case-deletion importance sampling estimators: central limit theorems and related results; citation_author=I Epifani, SN MacEachern, M Peruggia; citation_volume=2; citation_publication_date=2008; citation_pages=774-806; citation_doi=10.1214/08-EJS259; citation_id=CR6"/>

    <meta name="citation_reference" content="Gabry, J., Goodrich, B.: rstanarm: Bayesian applied regression modeling via Stan. R package version 2.10.0. (2016). 
                    http://mc-stan.org/interfaces/rstanarm
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=J. Am. Stat. Assoc.; citation_title=A predictive approach to model selection; citation_author=S Geisser, W Eddy; citation_volume=74; citation_publication_date=1979; citation_pages=153-160; citation_doi=10.1080/01621459.1979.10481632; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_title=Model determination using sampling-based methods; citation_inbook_title=Markov Chain Monte Carlo in Practice; citation_publication_date=1996; citation_pages=145-162; citation_id=CR9; citation_author=AE Gelfand; citation_publisher=Chapman and Hall"/>

    <meta name="citation_reference" content="citation_title=Model determination using predictive distributions with implementation via sampling-based methods; citation_inbook_title=Bayesian Statistics; citation_publication_date=1992; citation_pages=147-167; citation_id=CR10; citation_author=AE Gelfand; citation_author=DK Dey; citation_author=H Chang; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="citation_title=Bayesian Data Analysis; citation_publication_date=2013; citation_id=CR11; citation_author=A Gelman; citation_author=JB Carlin; citation_author=HS Stern; citation_author=DB Dunson; citation_author=A Vehtari; citation_author=DB Rubin; citation_publisher=CRC Press"/>

    <meta name="citation_reference" content="citation_title=Data Analysis Using Regression and Multilevel/Hierarchical Models; citation_publication_date=2007; citation_id=CR12; citation_author=A Gelman; citation_author=J Hill; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Stat. Comput.; citation_title=Understanding predictive information criteria for Bayesian models; citation_author=A Gelman, J Hwang, A Vehtari; citation_volume=24; citation_publication_date=2014; citation_pages=997-1016; citation_doi=10.1007/s11222-013-9416-2; citation_id=CR13"/>

    <meta name="citation_reference" content="Gneiting, T., Raftery, A.E.: Strictly proper scoring rules, prediction, and estimation. J. Am. Stat. Assoc. 102, 359&#8211;378 (2007)"/>

    <meta name="citation_reference" content="citation_journal_title=Stat. Sci.; citation_title=Bayesian model averaging; citation_author=J Hoeting, D Madigan, AE Raftery, C Volinsky; citation_volume=14; citation_publication_date=1999; citation_pages=382-417; citation_doi=10.1214/ss/1009212519; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=J. Mach. Learn. Res.; citation_title=The no-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo; citation_author=MD Hoffman, A Gelman; citation_volume=15; citation_publication_date=2014; citation_pages=1593-1623; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=J. Comput. Graph. Stat.; citation_title=Truncated importance sampling; citation_author=EL Ionides; citation_volume=17; citation_publication_date=2008; citation_pages=295-311; citation_doi=10.1198/106186008X320456; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=J. Econom.; citation_title=Testing the assumptions behind importance sampling; citation_author=SJ Koopman, N Shephard, D Creal; citation_volume=149; citation_publication_date=2009; citation_pages=2-11; citation_doi=10.1016/j.jeconom.2008.10.002; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=J. Am. Stat. Assoc.; citation_title=On the variability of case-deletion importance sampling weights in the Bayesian linear model; citation_author=M Peruggia; citation_volume=92; citation_publication_date=1997; citation_pages=199-207; citation_doi=10.1080/01621459.1997.10473617; citation_id=CR19"/>

    <meta name="citation_reference" content="Piironen, J., Vehtari, A.: Comparison of Bayesian predictive methods for model selection. Stat. Comput. (2016) (In press). 
                    http://link.springer.com/article/10.1007/s11222-016-9649-y
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Biostatistics; citation_title=Penalized loss functions for Bayesian model comparison; citation_author=M Plummer; citation_volume=9; citation_publication_date=2008; citation_pages=523-539; citation_doi=10.1093/biostatistics/kxm049; citation_id=CR21"/>

    <meta name="citation_reference" content="R Core Team: R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria (2016). 
                    https://www.R-project.org/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=J. Educ. Stat.; citation_title=Estimation in parallel randomized experiments; citation_author=DB Rubin; citation_volume=6; citation_publication_date=1981; citation_pages=377-401; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J. R. Stat. Soc. B; citation_title=Bayesian measures of model complexity and fit; citation_author=DJ Spiegelhalter, NG Best, BP Carlin, A Linde; citation_volume=64; citation_publication_date=2002; citation_pages=583-639; citation_doi=10.1111/1467-9868.00353; citation_id=CR24"/>

    <meta name="citation_reference" content="Spiegelhalter, D., Thomas, A., Best, N., Gilks, W., Lunn, D.: BUGS: Bayesian inference using Gibbs sampling. MRC Biostatistics Unit, Cambridge, England (1994, 2003). 
                    http://www.mrc-bsu.cam.ac.uk/bugs/
                    
                  
                        "/>

    <meta name="citation_reference" content="Stan Development Team: The Stan C++ Library, version 2.10.0 (2016a). 
                    http://mc-stan.org/
                    
                  
                        "/>

    <meta name="citation_reference" content="Stan Development Team: RStan: the R interface to Stan, version 2.10.1 (2016b). 
                    http://mc-stan.org/interfaces/rstan.html
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=J. R. Stat. Soc. B; citation_title=An asymptotic equivalence of choice of model cross-validation and Akaike&#8217;s criterion; citation_author=M Stone; citation_volume=36; citation_publication_date=1977; citation_pages=44-47; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Stat. Neerl.; citation_title=DIC in variable selection; citation_author=A Linde; citation_volume=1; citation_publication_date=2005; citation_pages=45-56; citation_doi=10.1111/j.1467-9574.2005.00278.x; citation_id=CR29"/>

    <meta name="citation_reference" content="Vehtari, A., Gelman, A.: Pareto smoothed importance sampling (2015). 
                    arXiv:1507.02646
                    
                  
                        "/>

    <meta name="citation_reference" content="Vehtari, A., Gelman, A., Gabry, J.: loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models. R package version 0.1.6 (2016a). 
                    https://github.com/stan-dev/loo
                    
                  
                        "/>

    <meta name="citation_reference" content="Vehtari, A., Mononen, T., Tolvanen, V., Sivula, T., Winther, O.: Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. J. Mach. Learn. Res. 17, 1&#8211;38 (2016b)"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Bayesian model assessment and comparison using cross-validation predictive densities; citation_author=A Vehtari, J Lampinen; citation_volume=14; citation_publication_date=2002; citation_pages=2439-2468; citation_doi=10.1162/08997660260293292; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Stat. Surv.; citation_title=A survey of Bayesian predictive methods for model assessment, selection and comparison; citation_author=A Vehtari, J Ojanen; citation_volume=6; citation_publication_date=2012; citation_pages=142-228; citation_doi=10.1214/12-SS102; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Bayesian Anal.; citation_title=Laplace approximation for logistic Gaussian process density estimation and regression; citation_author=A Vehtari, J Riihim&#228;ki; citation_volume=9; citation_publication_date=2014; citation_pages=425-448; citation_doi=10.1214/14-BA872; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=J. Mach. Learn. Res.; citation_title=Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory; citation_author=S Watanabe; citation_volume=11; citation_publication_date=2010; citation_pages=3571-3594; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=Technometrics; citation_title=A new and efficient estimation method for the generalized Pareto distribution; citation_author=J Zhang, MA Stephens; citation_volume=51; citation_publication_date=2009; citation_pages=316-325; citation_doi=10.1198/tech.2009.08017; citation_id=CR37"/>

    <meta name="citation_author" content="Aki Vehtari"/>

    <meta name="citation_author_email" content="aki.vehtari@aalto.fi"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Helsinki Institute for Information Technology HIIT, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Andrew Gelman"/>

    <meta name="citation_author_email" content="gelman@stat.columbia.edu"/>

    <meta name="citation_author_institution" content="Department of Statistics, Columbia University, New York, USA"/>

    <meta name="citation_author" content="Jonah Gabry"/>

    <meta name="citation_author_institution" content="Department of Statistics, Columbia University, New York, USA"/>

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Practical Bayesian model evaluation using leave-one-out cross-validati"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/11222/27/5.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s11222-016-9696-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2017/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s11222-016-9696-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Statistics and Computing"/>
        <meta property="og:title" content="Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC"/>
        <meta property="og:description" content="Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/11222.jpg"/>
    

    <title>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-c36432aacc.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-2ab899f004.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"SE","doi":"10.1007-s11222-016-9696-4","Journal Title":"Statistics and Computing","Journal Id":11222,"Keywords":"Bayesian computation, Leave-one-out cross-validation (LOO), \n                        K-fold cross-validation, Widely applicable information criterion (WAIC), Stan, Pareto smoothed importance sampling (PSIS)","kwrd":["Bayesian_computation","Leave-one-out_cross-validation_(LOO)","\n________________________K-fold_cross-validation","Widely_applicable_information_criterion_(WAIC)","Stan","Pareto_smoothed_importance_sampling_(PSIS)"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate","doNotAutoAssociate","doNotAutoAssociate","doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["2000492649","3000092005","3000169192","3000183675","3000506941","3002277247","3003089239"],"businessPartnerIDString":"2000492649|3000092005|3000169192|3000183675|3000506941|3002277247|3003089239"}},"Access Type":"subscription","Bpids":"2000492649, 3000092005, 3000169192, 3000183675, 3000506941, 3002277247, 3003089239","Bpnames":"Stockholm University, National Library of Sweden BIBSAM - Fully OA, BIBSAM, 1018 BIBSAM SWED, BIBSAM OJA 2011, National Library of Sweden (Kungliga Biblioteket), SpringerMaterials Tester","BPID":["2000492649","3000092005","3000169192","3000183675","3000506941","3002277247","3003089239"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s11222-016-9696-4","Full HTML":"Y","Subject Codes":["SCS","SCS12008","SCI21000","SCS11001","SCI17036"],"pmc":["S","S12008","I21000","S11001","I17036"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-1375","pissn":"0960-3174"},"type":"Article","category":{"pmc":{"primarySubject":"Statistics","primarySubjectCode":"S","secondarySubjects":{"1":"Statistics and Computing/Statistics Programs","2":"Artificial Intelligence","3":"Statistical Theory and Methods","4":"Probability and Statistics in Computer Science"},"secondarySubjectCodes":{"1":"S12008","2":"I21000","3":"S11001","4":"I17036"}},"sucode":"SC10"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s11222-016-9696-4","Page":"article"}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return !(scriptEl.hasOwnProperty('noModule')) && scriptEl.hasOwnProperty('onbeforeload');
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5a82bc1fe4.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-cba6a3f7dc.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-0c761bb6f7.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-db63800f6b.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-cda72a2f15.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-cfa69aa341.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
<div class="c-ad c-ad--LB1" data-test="springer-doubleclick-ad">
    <div class="c-ad c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/11222/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=9696;"></div>
    </div>
</div>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="true"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11222-016-9696-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="c-banner c-banner--marketing">
	<div class="u-container">
		<p class="u-ma-0">
			We'd like to understand how you use our websites in order to improve them.
			<a class="c-banner__link u-underline"
				href="https://www.surveymonkey.co.uk/r/F2JMQ6L" data-track="click"
				data-track-action="Survey click" data-track-category="Blue Banner"
				data-track-label=10.1007/s11222-016-9696-4>Register your interest.</a>
		</p>
	</div>
</div>
    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">

        <main class="c-article-main-column u-float-left js-main-column">
            <div id="pdflink-container" class="download-article test-pdf-link">
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s11222-016-9696-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-category="article body" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    
</div>

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-category="article body" data-track-label="link">Published: <time datetime="2016-08-30" itemprop="datePublished">30 August 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-1" data-corresp-id="c1">Aki Vehtari<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="0000000108389418, grid.5373.2, Department of Computer Science, Helsinki Institute for Information Technology HIIT, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-2">Andrew Gelman</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Columbia University" /><meta itemprop="address" content="0000000419368729, grid.21729.3f, Department of Statistics, Columbia University, New York, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-3">Jonah Gabry</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Columbia University" /><meta itemprop="address" content="0000000419368729, grid.21729.3f, Department of Statistics, Columbia University, New York, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/11222"><i data-test="journal-title">Statistics and Computing</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 27</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">1413</span>–<span itemprop="pageEnd">1432</span>(<span data-test="article-publication-year">2017</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-inline">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">13k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">470 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">42 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs11222-016-9696-4/metrics" data-track="click" data-track-action="view metrics" data-track-category="article body" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called <span class="u-monospace">loo</span> and demonstrate using models fit with the Bayesian inference package Stan.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>After fitting a Bayesian model we often want to measure its predictive accuracy, for its own sake or for purposes of model comparison, selection, or averaging (Geisser and Eddy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Geisser, S., Eddy, W.: A predictive approach to model selection. J. Am. Stat. Assoc. 74, 153–160 (1979)" href="/article/10.1007/s11222-016-9696-4#ref-CR8" id="ref-link-section-d71734e376">1979</a>; Hoeting et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Hoeting, J., Madigan, D., Raftery, A.E., Volinsky, C.: Bayesian model averaging. Stat. Sci. 14, 382–417 (1999)" href="/article/10.1007/s11222-016-9696-4#ref-CR15" id="ref-link-section-d71734e379">1999</a>; Vehtari and Lampinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e382">2002</a>; Ando and Tsay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ando, T., Tsay, R.: Predictive likelihood for Bayesian model selection and averaging. Int. J. Forecast. 26, 744–763 (2010)" href="/article/10.1007/s11222-016-9696-4#ref-CR2" id="ref-link-section-d71734e385">2010</a>; Vehtari and Ojanen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Vehtari, A., Ojanen, J.: A survey of Bayesian predictive methods for model assessment, selection and comparison. Stat. Surv. 6, 142–228 (2012)" href="/article/10.1007/s11222-016-9696-4#ref-CR34" id="ref-link-section-d71734e388">2012</a>). Cross-validation and information criteria are two approaches to estimating out-of-sample predictive accuracy using within-sample fits (Akaike <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1973" title="Akaike, H.: Information theory and an extension of the maximum likelihood principle. In: Petrov, B.N., Csaki, F. (eds.) Proceedings of the Second International Symposium on Information Theory, pp. 267–281. Akademiai Kiado, Budapest (1973)" href="/article/10.1007/s11222-016-9696-4#ref-CR1" id="ref-link-section-d71734e392">1973</a>; Stone <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1977" title="Stone, M.: An asymptotic equivalence of choice of model cross-validation and Akaike’s criterion. J. R. Stat. Soc. B 36, 44–47 (1977)" href="/article/10.1007/s11222-016-9696-4#ref-CR28" id="ref-link-section-d71734e395">1977</a>). In this article we consider computations using the log-likelihood evaluated at the usual posterior simulations of the parameters. Computation time for the predictive accuracy measures should be negligible compared to the cost of fitting the model and obtaining posterior draws in the first place.</p><p>Exact cross-validation requires re-fitting the model with different training sets. Approximate leave-one-out cross-validation (LOO) can be computed easily using importance sampling (IS; Gelfand et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Gelfand, A.E., Dey, D.K., Chang, H.: Model determination using predictive distributions with implementation via sampling-based methods. In: Bernardo, J.M., Berger, J.O., Dawid, A.P., Smith, A.F.M. (eds.) Bayesian Statistics, 4th edn, pp. 147–167. Oxford University Press, Oxford (1992)" href="/article/10.1007/s11222-016-9696-4#ref-CR10" id="ref-link-section-d71734e401">1992</a>; Gelfand <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gelfand, A.E.: Model determination using sampling-based methods. In: Gilks, W.R., Richardson, S., Spiegelhalter, D.J. (eds.) Markov Chain Monte Carlo in Practice, pp. 145–162. Chapman and Hall, London (1996)" href="/article/10.1007/s11222-016-9696-4#ref-CR9" id="ref-link-section-d71734e404">1996</a>) but the resulting estimate is noisy, as the variance of the importance weights can be large or even infinite (Peruggia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J. Am. Stat. Assoc. 92, 199–207 (1997)" href="/article/10.1007/s11222-016-9696-4#ref-CR19" id="ref-link-section-d71734e407">1997</a>; Epifani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e410">2008</a>). Here we propose to use <i>Pareto smoothed importance sampling</i> (PSIS), a new approach that provides a more accurate and reliable estimate by fitting a Pareto distribution to the upper tail of the distribution of the importance weights. PSIS allows us to compute LOO using importance weights that would otherwise be unstable.</p><p>The widely applicable or Watanabe-Akaike information criterion (WAIC; Watanabe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Watanabe, S.: Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. J. Mach. Learn. Res. 11, 3571–3594 (2010)" href="/article/10.1007/s11222-016-9696-4#ref-CR36" id="ref-link-section-d71734e419">2010</a>) can be viewed as an improvement on the deviance information criterion (DIC) for Bayesian models. DIC has gained popularity in recent years, in part through its implementation in the graphical modeling package BUGS (Spiegelhalter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Spiegelhalter, D.J., Best, N.G., Carlin, B.P., van der Linde, A.: Bayesian measures of model complexity and fit. J. R. Stat. Soc. B 64, 583–639 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR24" id="ref-link-section-d71734e422">2002</a>; Spiegelhalter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994, 2003" title="Spiegelhalter, D., Thomas, A., Best, N., Gilks, W., Lunn, D.: BUGS: Bayesian inference using Gibbs sampling. MRC Biostatistics Unit, Cambridge, England (1994, 2003). &#xA;                    http://www.mrc-bsu.cam.ac.uk/bugs/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR25" id="ref-link-section-d71734e425">1994, 2003</a>), but it is known to have some problems, which arise in part from not being fully Bayesian in that it is based on a point estimate (van der Linde <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="van der Linde, A.: DIC in variable selection. Stat. Neerl. 1, 45–56 (2005)" href="/article/10.1007/s11222-016-9696-4#ref-CR29" id="ref-link-section-d71734e428">2005</a>; Plummer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Plummer, M.: Penalized loss functions for Bayesian model comparison. Biostatistics 9, 523–539 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR21" id="ref-link-section-d71734e431">2008</a>). For example, DIC can produce negative estimates of the effective number of parameters in a model and it is not defined for singular models. WAIC is fully Bayesian in that it uses the entire posterior distribution, and it is asymptotically equal to Bayesian cross-validation. Unlike DIC, WAIC is invariant to parametrization and also works for singular models.</p><p>Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. We provide diagnostics for both PSIS-LOO and WAIC which tell when these approximations are likely to have large errors and computationally more intensive methods such as <i>K</i>-fold cross-validation should be used. Fast and stable computation and diagnostics for PSIS-LOO allows safe use of this new method in routine statistical practice. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for the comparison of predictive errors between two models.</p><p>We implement the computations in a package for R (R Core Team <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="R Core Team: R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria (2016). &#xA;                    https://www.R-project.org/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR22" id="ref-link-section-d71734e444">2016</a>) called <span class="u-monospace">loo</span> (Vehtari et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016a" title="Vehtari, A., Gelman, A., Gabry, J.: loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models. R package version 0.1.6 (2016a). &#xA;                    https://github.com/stan-dev/loo&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR31" id="ref-link-section-d71734e450">2016a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Vehtari, A., Mononen, T., Tolvanen, V., Sivula, T., Winther, O.: Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. J. Mach. Learn. Res. 17, 1–38 (2016b)" href="/article/10.1007/s11222-016-9696-4#ref-CR32" id="ref-link-section-d71734e453">b</a>) and demonstrate using models fit with the Bayesian inference package Stan (Stan Development Team <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016a" title="Stan Development Team: The Stan C++ Library, version 2.10.0 (2016a). &#xA;                    http://mc-stan.org/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR26" id="ref-link-section-d71734e456">2016a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Stan Development Team: RStan: the R interface to Stan, version 2.10.1 (2016b). &#xA;                    http://mc-stan.org/interfaces/rstan.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR27" id="ref-link-section-d71734e460">b</a>).<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> All the computations are fast compared to the typical time required to fit the model in the first place. Although the examples provided in this paper all use Stan, the <span class="u-monospace">loo</span> package is independent of Stan and can be used with models estimated by other software packages or custom user-written algorithms.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Estimating out-of-sample pointwise predictive accuracy using posterior simulations</h2><div class="c-article-section__content" id="Sec2-content"><p>Consider data <span class="mathjax-tex">\(y_1,\ldots ,y_n\)</span>, modeled as independent given parameters <span class="mathjax-tex">\(\theta \)</span>; thus <span class="mathjax-tex">\(p(y|\theta )=\prod _{i=1}^n p(y_i | \theta )\)</span>. This formulation also encompasses latent variable models with <span class="mathjax-tex">\(p(y_i | f_i, \theta )\)</span>, where <span class="mathjax-tex">\(f_i\)</span> are latent variables. Also suppose we have a prior distribution <span class="mathjax-tex">\(p(\theta )\)</span>, thus yielding a posterior distribution <span class="mathjax-tex">\(p(\theta |y)\)</span> and a posterior predictive distribution <span class="mathjax-tex">\(p(\tilde{y}|y)=\int p(\tilde{y}_i|\theta )p(\theta |y)d\theta \)</span>. To maintain comparability with the given dataset and to get easier interpretation of the differences in scale of effective number of parameters, we define a measure of predictive accuracy for the <i>n</i> data points taken one at a time:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \hbox {elpd}= &amp; {} \hbox {expected log pointwise predictive density} \nonumber \\&amp;\quad \hbox {for a new dataset}\nonumber \\= &amp; {} \sum _{i=1}^n \int p_t(\tilde{y}_i) \log p(\tilde{y}_i|y) d\tilde{y}_i, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(p_t(\tilde{y}_i)\)</span> is the distribution representing the true data-generating process for <span class="mathjax-tex">\(\tilde{y}_i\)</span>. The <span class="mathjax-tex">\(p_t(\tilde{y}_i)\)</span>’s are unknown, and we will use cross-validation or WAIC to approximate (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ1">1</a>). In a regression, these distributions are also implicitly conditioned on any predictors in the model. See Vehtari and Ojanen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Vehtari, A., Ojanen, J.: A survey of Bayesian predictive methods for model assessment, selection and comparison. Stat. Surv. 6, 142–228 (2012)" href="/article/10.1007/s11222-016-9696-4#ref-CR34" id="ref-link-section-d71734e1204">2012</a>) for other approaches to approximating <span class="mathjax-tex">\(p_t(\tilde{y}_i)\)</span> and discussion of alternative prediction tasks.</p><p>Instead of the log predictive density <span class="mathjax-tex">\(\log p(\tilde{y}_i|y)\)</span>, other utility (or cost) functions <span class="mathjax-tex">\(u(p(\tilde{y}|y),\tilde{y})\)</span> could be used, such as classification error. Here we take the log score as the default for evaluating the predictive density (Geisser and Eddy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Geisser, S., Eddy, W.: A predictive approach to model selection. J. Am. Stat. Assoc. 74, 153–160 (1979)" href="/article/10.1007/s11222-016-9696-4#ref-CR8" id="ref-link-section-d71734e1371">1979</a>; Bernardo and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Bernardo, J.M., Smith A.F.M.: Bayesian Theory. Wiley, New York (1994)" href="/article/10.1007/s11222-016-9696-4#ref-CR4" id="ref-link-section-d71734e1374">1994</a>; Gneiting and Raftery <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Gneiting, T., Raftery, A.E.: Strictly proper scoring rules, prediction, and estimation. J. Am. Stat. Assoc. 102, 359–378 (2007)" href="/article/10.1007/s11222-016-9696-4#ref-CR14" id="ref-link-section-d71734e1377">2007</a>).</p><p>A helpful quantity in the analysis is</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathrm{lpd}= &amp; {} \text{ log } \text{ pointwise } \text{ predictive } \text{ density } \nonumber \\= &amp; {} \sum _{i=1}^n \log p(y_i|y)=\sum _{i=1}^n \log \int p(y_i|\theta )p(\theta |y)d\theta . \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>The lpd of observed data <i>y</i> is an overestimate of the elpd for future data (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ1">1</a>). To compute the lpd in practice, we can evaluate the expectation using draws from <span class="mathjax-tex">\(p_\mathrm{post}(\theta )\)</span>, the usual posterior simulations, which we label <span class="mathjax-tex">\(\theta ^s,s=1,\ldots ,S\)</span>:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \widehat{\mathrm{lpd}}= &amp; {} \text{ computed } \text{ log } \text{ pointwise } \text{ predictive } \text{ density } \nonumber \\= &amp; {} \sum _{i=1}^n \log \left( \frac{1}{S}\sum _{s=1}^S p(y_i|\theta ^s)\right) . \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                     <h3 class="c-article__sub-heading" id="Sec3">Leave-one-out cross-validation</h3><p>The Bayesian LOO estimate of out-of-sample predictive fit is</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ4_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} p(y_i|y_{-i})=\int p(y_i|\theta )p(\theta |y_{-i}) d\theta \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>is the leave-one-out predictive density given the data without the <i>i</i>th data point.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec4">Raw importance sampling</h4><p>As noted by Gelfand et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Gelfand, A.E., Dey, D.K., Chang, H.: Model determination using predictive distributions with implementation via sampling-based methods. In: Bernardo, J.M., Berger, J.O., Dawid, A.P., Smith, A.F.M. (eds.) Bayesian Statistics, 4th edn, pp. 147–167. Oxford University Press, Oxford (1992)" href="/article/10.1007/s11222-016-9696-4#ref-CR10" id="ref-link-section-d71734e2022">1992</a>), if the <i>n</i> points are conditionally independent in the data model we can then evaluate (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ5">5</a>) with draws <span class="mathjax-tex">\(\theta ^s\)</span> from the full posterior <span class="mathjax-tex">\(p(\theta |y)\)</span> using importance ratios</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} r_i^s=\frac{1}{p(y_i|\theta ^s)} \propto \frac{p(\theta ^s|y_{-i})}{p(\theta ^s|y)} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>to get the importance sampling leave-one-out (IS-LOO) predictive distribution,</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} p(\tilde{y}_i|y_{-i})\approx \frac{\sum _{s=1}^S r_i^s p(\tilde{y}_i|\theta ^s)}{\sum _{s=1}^S r_i^s}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>Evaluating this LOO log predictive density at the held-out data point <span class="mathjax-tex">\(y_i\)</span>, we get</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} p(y_i|y_{-i})\approx \frac{1}{\frac{1}{S}\sum _{s=1}^S\frac{1}{p(y_i|\theta ^s)}}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>However, the posterior <span class="mathjax-tex">\(p(\theta |y)\)</span> is likely to have a smaller variance and thinner tails than the leave-one-out distributions <span class="mathjax-tex">\(p(\theta |y_{-i})\)</span>, and thus a direct use of (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ8">8</a>) induces instability because the importance ratios can have high or infinite variance.</p><p>For simple models the variance of the importance weights may be computed analytically. The necessary and sufficient conditions for the variance of the case-deletion importance sampling weights to be finite for a Bayesian linear model are given by Peruggia (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J. Am. Stat. Assoc. 92, 199–207 (1997)" href="/article/10.1007/s11222-016-9696-4#ref-CR19" id="ref-link-section-d71734e2662">1997</a>). Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e2665">2008</a>) extend the analytical results to generalized linear models and non-linear Michaelis-Menten models. However, these conditions can not be computed analytically in general.</p><p>
Koopman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Koopman, S.J., Shephard, N., Creal, D.: Testing the assumptions behind importance sampling. J. Econom. 149, 2–11 (2009)" href="/article/10.1007/s11222-016-9696-4#ref-CR18" id="ref-link-section-d71734e2671">2009</a>) propose to use the maximum likelihood fit of the generalized Pareto distribution to the upper tail of the distribution of the importance ratios and use the fitted parameters to form a test for whether the variance of the importance ratios is finite. If the hypothesis test suggests the variance is infinite then they abandon importance sampling.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Truncated importance sampling</h4><p>
Ionides (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ionides, E.L.: Truncated importance sampling. J. Comput. Graph. Stat. 17, 295–311 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR17" id="ref-link-section-d71734e2682">2008</a>) proposes a modification of importance sampling where the raw importance ratios <span class="mathjax-tex">\(r^s\)</span> are replaced by truncated weights</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} w^s = \min (r^s,\sqrt{S}\bar{r}), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>where <span class="mathjax-tex">\(\bar{r}=\frac{1}{S}\sum _{s=1}^Sr^s\)</span>. Ionides (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ionides, E.L.: Truncated importance sampling. J. Comput. Graph. Stat. 17, 295–311 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR17" id="ref-link-section-d71734e2861">2008</a>) proves that the variance of the truncated importance sampling weights is guaranteed to be finite, and provides theoretical and experimental results showing that truncation using the threshold <span class="mathjax-tex">\(\sqrt{S}\bar{r}\)</span> gives an importance sampling estimate with a mean square error close to an estimate with a case specific optimal truncation level. The downside of the truncation is that it introduces a bias, which can be large as we demonstrate in our experiments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Pareto smoothed importance sampling</h4><p>We can improve the LOO estimate using Pareto smoothed importance sampling (PSIS; Vehtari and Gelman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Vehtari, A., Gelman, A.: Pareto smoothed importance sampling (2015). &#xA;                    arXiv:1507.02646&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR30" id="ref-link-section-d71734e2912">2015</a>), which applies a smoothing procedure to the importance weights. We briefly review the motivation and steps of PSIS here, before moving on to focus on the goals of using and evaluating predictive information criteria.</p><p>As noted above, the distribution of the importance weights used in LOO may have a long right tail. We use the empirical Bayes estimate of Zhang and Stephens (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zhang, J., Stephens, M.A.: A new and efficient estimation method for the generalized Pareto distribution. Technometrics 51, 316–325 (2009)" href="/article/10.1007/s11222-016-9696-4#ref-CR37" id="ref-link-section-d71734e2918">2009</a>) to fit a generalized Pareto distribution to the tail (20 % largest importance ratios). By examining the shape parameter <i>k</i> of the fitted Pareto distribution, we are able to obtain sample based estimates of the existence of the moments (Koopman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Koopman, S.J., Shephard, N., Creal, D.: Testing the assumptions behind importance sampling. J. Econom. 149, 2–11 (2009)" href="/article/10.1007/s11222-016-9696-4#ref-CR18" id="ref-link-section-d71734e2924">2009</a>). This extends the diagnostic approach of Peruggia (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J. Am. Stat. Assoc. 92, 199–207 (1997)" href="/article/10.1007/s11222-016-9696-4#ref-CR19" id="ref-link-section-d71734e2927">1997</a>) and Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e2930">2008</a>) to be used routinely with IS-LOO for any model with a factorizing likelihood.</p><p>
Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e2936">2008</a>) show that when estimating the leave-one-out predictive density, the central limit theorem holds if the distribution of the weights has finite variance. These results can be extended via the generalized central limit theorem for stable distributions. Thus, even if the variance of the importance weight distribution is infinite, if the mean exists then the accuracy of the estimate improves as additional posterior draws are obtained.</p><p>When the tail of the weight distribution is long, a direct use of importance sampling is sensitive to one or few largest values. By fitting a generalized Pareto distribution to the upper tail of the importance weights, we smooth these values. The procedure goes as follows:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Fit the generalized Pareto distribution to the 20% largest importance ratios <span class="mathjax-tex">\(r_s\)</span> as computed in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ6">6</a>). The computation is done separately for each held-out data point <i>i</i>. In simulation experiments with thousands and tens of thousands of draws, we have found that the fit is not sensitive to the specific cutoff value (for a consistent estimation, the proportion of the samples above the cutoff should get smaller when the number of draws increases).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Stabilize the importance ratios by replacing the <i>M</i> largest ratios by the expected values of the order statistics of the fitted generalized Pareto distribution </p><div id="Equ25" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} F^{-1}\left( \frac{z-1/2}{M}\right) , \quad z=1,\ldots ,M, \end{aligned}$$</span></div></div><p> where <i>M</i> is the number of simulation draws used to fit the Pareto (in this case, <span class="mathjax-tex">\(M=0.2\,S\)</span>) and <span class="mathjax-tex">\(F^{-1}\)</span> is the inverse-CDF of the generalized Pareto distribution. Label these new weights as <span class="mathjax-tex">\(\tilde{w}_i^s\)</span> where, again, <i>s</i> indexes the simulation draws and <i>i</i> indexes the data points; thus, for each <i>i</i> there is a distinct vector of <i>S</i> weights.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>To guarantee finite variance of the estimate, truncate each vector of weights at <span class="mathjax-tex">\(S^{3/4}\bar{w}_i\)</span>, where <span class="mathjax-tex">\(\bar{w}_i\)</span> is the average of the <i>S</i> smoothed weights corresponding to the distribution holding out data point <i>i</i>. Finally, label these truncated weights as <span class="mathjax-tex">\(w^s_i\)</span>.</p>
                        
                      </li>
                    </ol><p>The above steps must be performed for each data point <i>i</i>. The result is a vector of weights <span class="mathjax-tex">\(w_i^s, s=1,\ldots ,S\)</span>, for each <i>i</i>, which in general should be better behaved than the raw importance ratios <span class="mathjax-tex">\(r_i^s\)</span> from which they are constructed.</p><p>The results can then be combined to compute the desired LOO estimates. The PSIS estimate of the LOO expected log pointwise predictive density is</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ10_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>The estimated shape parameter <span class="mathjax-tex">\({\hat{k}}\)</span> of the generalized Pareto distribution can be used to assess the reliability of the estimate:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>If <span class="mathjax-tex">\(k&lt;\frac{1}{2}\)</span>, the variance of the raw importance ratios is finite, the central limit theorem holds, and the estimate converges quickly.</p>
                      </li>
                      <li>
                        <p>If <i>k</i> is between <span class="mathjax-tex">\(\frac{1}{2}\)</span> and 1, the variance of the raw importance ratios is infinite but the mean exists, the generalized central limit theorem for stable distributions holds, and the convergence of the estimate is slower. The variance of the PSIS estimate is finite but may be large.</p>
                      </li>
                      <li>
                        <p>If <span class="mathjax-tex">\(k&gt;1\)</span>, the variance and the mean of the raw ratios distribution do not exist. The variance of the PSIS estimate is finite but may be large.</p>
                      </li>
                    </ul><p>If the estimated tail shape parameter <span class="mathjax-tex">\({\hat{k}}\)</span> exceeds 0.5, the user should be warned, although in practice we have observed good performance for values of <span class="mathjax-tex">\({\hat{k}}\)</span> up to 0.7. Even if the PSIS estimate has a finite variance, when <span class="mathjax-tex">\({\hat{k}}\)</span> exceeds 0.7 the user should consider sampling directly from <span class="mathjax-tex">\(p(\theta ^s|y_{-i})\)</span> for the problematic <i>i</i>, use <i>K</i>-fold cross-validation (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec8">2.3</a>), or use a more robust model.</p><p>The additional computational cost of sampling directly from each <span class="mathjax-tex">\(p(\theta ^s|y_{-i})\)</span> is approximately the same as sampling from the full posterior, but it is recommended if the number of problematic data points is not too high.</p><p>A more robust model may also help because importance sampling is less likely to work well if the marginal posterior <span class="mathjax-tex">\(p(\theta ^s|y)\)</span> and LOO posterior <span class="mathjax-tex">\(p(\theta ^s|y_{-i})\)</span> are very different. This is more likely to happen with a non-robust model and highly influential observations. A robust model may reduce the sensitivity to one or several highly influential observations, as we show in the examples in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec11">4</a>.</p><h3 class="c-article__sub-heading" id="Sec7">WAIC</h3><p>WAIC (Watanabe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Watanabe, S.: Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. J. Mach. Learn. Res. 11, 3571–3594 (2010)" href="/article/10.1007/s11222-016-9696-4#ref-CR36" id="ref-link-section-d71734e3852">2010</a>) is an alternative approach to estimating the expected log pointwise predictive density and is defined as</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ11_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <span class="mathjax-tex">\(\widehat{p}_\mathrm{waic}\)</span> is the estimated effective number of parameters and is computed based on the definition<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>
                           </p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} p_\mathrm{waic} =\sum _{i=1}^n \text{ var }_\mathrm{post} \left( \log p(y_i|\theta )\right) , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>which we can calculate using the posterior variance of the log predictive density for each data point <span class="mathjax-tex">\(y_i\)</span>, that is, <span class="mathjax-tex">\(V_{s=1}^S \log p(y_i|\theta ^s)\)</span>, where <span class="mathjax-tex">\(V_{s=1}^S\)</span> represents the sample variance, <span class="mathjax-tex">\(V_{s=1}^S a_s = \frac{1}{S-1}\sum _{s=1}^S (a_s - \bar{a})^2\)</span>. Summing over all the data points <span class="mathjax-tex">\(y_i\)</span> gives a simulation-estimated effective number of parameters,</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \widehat{p}_\mathrm{waic} = \sum _{i=1}^n V_{s=1}^S\left( \log p(y_i|\theta ^s)\right) . \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>For DIC, there is a similar variance-based computation of the number of parameters that is notoriously unreliable, but the WAIC version is more stable because it computes the variance separately for each data point and then takes the sum; the summing yields stability.</p><p>The <i>effective number of parameters</i> 
                           <span class="mathjax-tex">\(\widehat{p}_\mathrm{waic}\)</span> can be used as measure of complexity of the model, but it should not be overinterpreted, as the original goal is to estimate the difference between lpd and elpd. As shown by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. 24, 997–1016 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR13" id="ref-link-section-d71734e4512">2014</a>) and demonstrated also in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec11">4</a>, in the case of a weak prior, <span class="mathjax-tex">\(\widehat{p}_\mathrm{waic}\)</span> can severely underestimate the difference between lpd and elpd. For <span class="mathjax-tex">\(\widehat{p}_\mathrm{waic}\)</span> there is no similar theory as for the moments of the importance sampling weight distribution, but based on our simulation experiments it seems that <span class="mathjax-tex">\(\widehat{p}_\mathrm{waic}\)</span> is unreliable if any of the terms <span class="mathjax-tex">\(V_{s=1}^S \log p(y_i|\theta ^s)\)</span> exceeds 0.4.</p><p>The different behavior of LOO and WAIC seen in the experiments can be understood by comparing Taylor series approximations. By defining a generating function of functional cumulants,</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} F(\alpha )=\sum _{i=1}^n\log E_\mathrm{post}(p(y_i|\theta )^\alpha ), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p>and applying a Taylor expansion of <span class="mathjax-tex">\(F(\alpha )\)</span> around 0 with <span class="mathjax-tex">\(\alpha =-1\)</span> we obtain an expansion of <span class="mathjax-tex">\(\mathrm{lpd}_\mathrm{loo}\)</span>
                           </p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ15_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>From the definition of <span class="mathjax-tex">\(F(\alpha )\)</span> we get</p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ16_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (16)
                </div></div><p>Furthermore</p><div id="Equ17" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathrm{lpd}=F(1)=F'(0)+\frac{1}{2}F''(0)+\frac{1}{6}F^{(3)}(0)+ \sum _{i=4}^\infty \frac{F^{(i)}(0)}{i!}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (17)
                </div></div><p>and the expansion for WAIC is then</p><div id="Equ18" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \text {WAIC}&amp;=F(1)-F''(0)\nonumber \\&amp;=F'(0)-\frac{1}{2}F''(0)+\frac{1}{6}F^{(3)}(0)+ \sum _{i=4}^\infty \frac{F^{(i)}(0)}{i!}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (18)
                </div></div><p>The first three terms of the expansion of WAIC match the expansion of LOO, and the rest of the terms match the expansion of lpd. Watanabe (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Watanabe, S.: Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. J. Mach. Learn. Res. 11, 3571–3594 (2010)" href="/article/10.1007/s11222-016-9696-4#ref-CR36" id="ref-link-section-d71734e5385">2010</a>) argues that, asymptotically, the latter terms have negligible contribution and thus asymptotic equivalence with LOO is obtained. However, the error can be significant in the case of finite <i>n</i> and weak prior information as shown by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. 24, 997–1016 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR13" id="ref-link-section-d71734e5392">2014</a>), and demonstrated also in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec11">4</a>. If the higher order terms are not negligible, then WAIC is biased towards lpd. To reduce this bias it is possible to compute additional series terms, but computing higher moments using a finite posterior sample increases the variance of the estimate and, based on our experiments, it is more difficult to control the bias-variance tradeoff than in PSIS-LOO. WAIC’s larger bias compared to LOO is also demonstrated by Vehtari et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016a" title="Vehtari, A., Gelman, A., Gabry, J.: loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models. R package version 0.1.6 (2016a). &#xA;                    https://github.com/stan-dev/loo&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR31" id="ref-link-section-d71734e5398">2016a</a>, (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016b" title="Vehtari, A., Mononen, T., Tolvanen, V., Sivula, T., Winther, O.: Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. J. Mach. Learn. Res. 17, 1–38 (2016b)" href="/article/10.1007/s11222-016-9696-4#ref-CR32" id="ref-link-section-d71734e5401">2016b</a>) in the case of Gaussian processes with distributional posterior approximations. In the experiments we also demonstrate that we can use truncated IS-LOO with heavy truncation to obtain similar bias towards lpd and similar estimate variance as in WAIC.</p><h3 class="c-article__sub-heading" id="Sec8">
                           <i>K</i>-fold cross-validation</h3><p>In this paper we focus on leave-one-out cross-validation and WAIC, but, for statistical and computational reasons, it can make sense to cross-validate using <span class="mathjax-tex">\(K \ll n\)</span> hold-out sets. In some ways, <i>K</i>-fold cross-validation is simpler than leave-one-out cross-validation but in other ways it is not. <i>K</i>-fold cross-validation requires refitting the model <i>K</i> times which can be computationally expensive whereas approximative LOO methods, such as PSIS-LOO, require only one evaluation of the model.</p><p>If in PSIS-LOO <span class="mathjax-tex">\({\hat{k}}&gt;0.7\)</span> for a few <i>i</i> we recommend sampling directly from each corresponding <span class="mathjax-tex">\(p(\theta ^s|y_{-i})\)</span>, but if there are more than <i>K</i> problematic <i>i</i>, then we recommend checking the results using <i>K</i>-fold cross-validation. Vehtari and Lampinen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e5551">2002</a>) demonstrate cases where IS-LOO fails (according to effective sample size estimates instead of the <span class="mathjax-tex">\({\hat{k}}\)</span> diagnostic proposed here) for a large number of <i>i</i> and K-fold-CV produces more reliable results.</p><p>In Bayesian <i>K</i>-fold cross-validation, the data are partitioned into <i>K</i> subsets <span class="mathjax-tex">\(y_k\)</span>, for <span class="mathjax-tex">\(k=1,\ldots ,K\)</span>, and then the model is fit separately to each training set <span class="mathjax-tex">\(y_{(-k)}\)</span>, thus yielding a posterior distribution <span class="mathjax-tex">\(p_{\mathrm{post} (-k)}(\theta )=p(\theta |y_{(-k)})\)</span>. If the number of partitions is small (a typical value in the literature is <span class="mathjax-tex">\(K=10\)</span>), it is not so costly to simply re-fit the model separately to each training set. To maintain consistency with LOO and WAIC, we define predictive accuracy for each data point, so that the log predictive density for <span class="mathjax-tex">\(y_i\)</span>, if it is in subset <i>k</i>, is</p><div id="Equ19" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \log p(y_i|y_{(-k)})=\log {\int }p(y_i|\theta )p(\theta |y_{(-k)})d\theta , \quad i \in k. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (19)
                </div></div><p>Assuming the posterior distribution <span class="mathjax-tex">\(p(\theta |y_{(-k)})\)</span> is summarized by <i>S</i> simulation draws <span class="mathjax-tex">\(\theta ^{k,s}\)</span>, we calculate its log predictive density as</p><div id="Equ20" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \widehat{\mathrm{elpd}}_i = \log \left( \frac{1}{S}\sum _{s=1}^S p(y_i|\theta ^{k,s})\right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (20)
                </div></div><p>using the simulations corresponding to the subset <i>k</i> that contains data point <i>i</i>. We then sum to get the estimate</p><div id="Equ21" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \widehat{\mathrm{elpd}}_\mathrm{xval}=\sum _{i=1}^n \widehat{\mathrm{elpd}}_i. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (21)
                </div></div><p>There remains a bias as the model is learning from a fraction <span class="mathjax-tex">\(\frac{1}{K}\)</span> less of the data. Methods for correcting this bias exist but are rarely used as they can increase the variance, and if <span class="mathjax-tex">\(K \ge 10\)</span> the size of the bias is typically small compared to the variance of the estimate (Vehtari and Lampinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e6332">2002</a>). In our experiments, exact LOO is the same as <i>K</i>-fold-CV with <span class="mathjax-tex">\(K=N\)</span> and we also analyze the effect of this bias and bias correction in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec13">4.2</a>.</p><p>For <i>K</i>-fold cross-validation, if the subjects are exchangeable, that is, the order does not contain information, then there is no need for random selection. If the order does contain information, e.g. in survival studies the later patients have shorter follow-ups, then randomization is often useful.</p><p>In most cases we recommend partitioning the data into subsets by randomly permuting the observations and then systemically dividing them into <i>K</i> subgroups. If the subjects are exchangeable, that is, the order does not contain information, then there is no need for random selection, but if the order does contain information, e.g. in survival studies the later patients have shorter follow-ups, then randomization is useful. In some cases it may be useful to stratify to obtain better balance among groups. See Vehtari and Lampinen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e6380">2002</a>), Arolot and Celisse (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Arolot, S., Celisse, A.: A survey of cross-validation procedures for model selection. Stat. Surv. 4, 40–79 (2010)" href="/article/10.1007/s11222-016-9696-4#ref-CR3" id="ref-link-section-d71734e6383">2010</a>), and Vehtari and Ojanen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Vehtari, A., Ojanen, J.: A survey of Bayesian predictive methods for model assessment, selection and comparison. Stat. Surv. 6, 142–228 (2012)" href="/article/10.1007/s11222-016-9696-4#ref-CR34" id="ref-link-section-d71734e6386">2012</a>) for further discussion of these points.</p><p>As the data can be divided in many ways into <i>K</i> groups it introduces additional variance in the estimates, which is also evident from our experiments. This variance can be reduced by repeating <i>K</i>-fold-CV several times with different permutations in the data division, but this will further increase the computational cost.</p><h3 class="c-article__sub-heading" id="Sec9">Data division</h3><p>The purpose of using LOO or WAIC is to estimate the accuracy of the predictive distribution <span class="mathjax-tex">\(p(\tilde{y}_i|y)\)</span>. Computation of PSIS-LOO and WAIC (and AIC and DIC) is based on computing terms <span class="mathjax-tex">\(\log p(y_i|y)=\log \int p(y_i|\theta )p(\theta |y)\)</span> assuming some agreed-upon division of the data <i>y</i> into individual data points <span class="mathjax-tex">\(y_i\)</span>. Although often <span class="mathjax-tex">\(y_i\)</span> will denote a single scalar observation, in the case of hierarchical data, it may denote a group of observations. For example, in cognitive or medical studies we may be interested in prediction for a new subject (or patient), and thus it is natural in cross-validation to consider an approach where <span class="mathjax-tex">\(y_i\)</span> would denote all observations for a single subject and <span class="mathjax-tex">\(y_{-i}\)</span> would denote the observations for all the other subjects. In theory, we can use PSIS-LOO and WAIC in this case, too, but as the number of observations per subject increases it is more likely that they will not work as well. The fact that importance sampling is difficult in higher dimensions is well known and is demonstrated for IS-LOO by Vehtari and Lampinen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e6653">2002</a>) and for PSIS by Vehtari and Gelman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Vehtari, A., Gelman, A.: Pareto smoothed importance sampling (2015). &#xA;                    arXiv:1507.02646&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR30" id="ref-link-section-d71734e6656">2015</a>). The same problem can also be shown to hold for WAIC. If diagnostics warn about the reliability of PSIS-LOO (or WAIC), then <i>K</i>-fold cross-validation can be used by taking into account the hierarchical structure in the data when doing the data division as demonstrated, for example, by Vehtari and Lampinen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e6662">2002</a>).</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Implementation in Stan</h2><div class="c-article-section__content" id="Sec10-content"><p>We have set up code to implement LOO, WAIC, and <i>K</i>-fold cross-validation in R and Stan so that users will have a quick and convenient way to assess and compare model fits. Implementation is not automatic, though, because of the need to compute the separate factors <span class="mathjax-tex">\(p(y_i|\theta )\)</span> in the likelihood. Stan works with the joint density and in its usual computations does not “know” which parts come from the prior and which from the likelihood. Nor does Stan in general make use of any factorization of the likelihood into pieces corresponding to each data point. Thus, to compute these measures of predictive fit in Stan, the user needs to explicitly code the factors of the likelihood (actually, the terms of the log-likelihood) as a vector. We can then pull apart the separate terms and compute cross-validation and WAIC at the end, after all simulations have been collected. Sample code for carrying out this procedure using Stan and the <span class="u-monospace">loo</span> R package is provided in Appendix. This code can be adapted to apply our procedure in other computing languages.</p><p>Although the implementation is not automatic when writing custom Stan programs, we can create implementations that are automatic for users of our new <span class="u-monospace">rstanarm</span> R package (Gabry and Goodrich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Gabry, J., Goodrich, B.: rstanarm: Bayesian applied regression modeling via Stan. R package version 2.10.0. (2016). &#xA;                    http://mc-stan.org/interfaces/rstanarm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR7" id="ref-link-section-d71734e6728">2016</a>). <span class="u-monospace">rstanarm</span> provides a high-level interface to Stan that enables the user to specify many of the most common applied Bayesian regression models using standard R modeling syntax (e.g. like that of <span class="u-monospace">glm</span>). The models are then estimated using Stan’s algorithms and the results are returned to the user in a form similar to the fitted model objects to which R users are accustomed. For the models implemented in <span class="u-monospace">rstanarm</span>, we have preprogrammed many tasks, including computing and saving the pointwise predictive measures and importance ratios which we use to compute WAIC and PSIS-LOO. The <span class="u-monospace">loo</span> method for <span class="u-monospace">rstanarm</span> models requires no additional programming from the user after fitting a model, as we can compute all of the needed quantities internally from the contents of the fitted model object and then pass them to the functions in the <span class="u-monospace">loo</span> package. Examples of using <span class="u-monospace">loo</span> with <span class="u-monospace">rstanarm</span> can be found in the <span class="u-monospace">rstanarm</span> vignettes, and we also provide an example in Appendix 3 of this paper.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Examples</h2><div class="c-article-section__content" id="Sec11-content"><p>We illustrate with six simple examples: two examples from our earlier research in computing the effective number of parameters in a hierarchical model, three examples that were used by Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e6767">2008</a>) to illustrate the estimation of the variance of the weight distribution, and one example of a multilevel regression from our earlier applied research. For each example we used the Stan default of 4 chains run for 1000 warmup and 1000 post-warmup iterations, yielding a total of 4000 saved simulation draws. With Gibbs sampling or random-walk Metropolis, 4000 is not a large number of simulation draws. The algorithm used by Stan is Hamiltonian Monte Carlo with No-U-Turn-Sampling (Hoffman and Gelman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Hoffman, M.D., Gelman, A.: The no-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res. 15, 1593–1623 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR16" id="ref-link-section-d71734e6770">2014</a>), which is much more efficient, and 1000 is already more than sufficient in many real-world settings. In these examples we followed standard practice and monitored convergence and effective sample sizes as recommended by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.: Bayesian Data Analysis, 3rd edn. CRC Press, London (2013)" href="/article/10.1007/s11222-016-9696-4#ref-CR11" id="ref-link-section-d71734e6773">2013</a>). We performed 100 independent replications of all experiments to obtain estimates of variation. For the exact LOO results and convergence plots we run longer chains to obtain a total of 100,000 draws (except for the radon example which is much slower to run).</p><h3 class="c-article__sub-heading" id="Sec12">Example: Scaled 8 schools</h3>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 In a controlled study, independent randomized experiments were conducted in 8 different high schools to estimate the effect of special preparation for college admission tests</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="/article/10.1007/s11222-016-9696-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>8 Schools example: <b>a</b> WAIC, Truncated Importance Sampling LOO, Importance Sampling LOO, Pareto Smoothed Importance Sampling LOO, and exact LOO (which in this case corresponds to eightfold-CV); <b>b</b> estimated effective number of parameters for each of these measures; <b>c</b> tail shape <span class="mathjax-tex">\(\hat{k}\)</span> for the importance weights; and <b>d</b> the posterior variances of the log predictive densities, for scaled versions of the 8 schools data (the original observations <i>y</i> have been multiplied by a common factor). We consider scaling factors ranging from 0.1 (corresponding to near-zero variation of the underlying parameters among the schools) to 4 (implying that the true effects in the schools vary by much more than their standard errors of measurement). As the scaling increases, eventually LOO approximations and WAIC fail to approximate exact LOO as the leave-one-out posteriors are not close to the full posterior. When the estimated tail shape <span class="mathjax-tex">\(\hat{k}\)</span> exceeds 1, the importance-weighted LOO approximations start to fail. When posterior variances of the log predictive densities exceeds 0.4, WAIC starts to fail. PSIS-LOO performs the best among the approximations considered here</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For our first example we take an analysis of an education experiment used by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. 24, 997–1016 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR13" id="ref-link-section-d71734e7165">2014</a>) to demonstrate the use of information criteria for hierarchical Bayesian models.</p><p>The goal of the study was to measure the effects of a test preparation program conducted in eight different high schools in New Jersey. A separate randomized experiment was conducted in each school, and the administrators of each school implemented the program in their own way. Rubin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Rubin, D.B.: Estimation in parallel randomized experiments. J. Educ. Stat. 6, 377–401 (1981)" href="/article/10.1007/s11222-016-9696-4#ref-CR23" id="ref-link-section-d71734e7171">1981</a>) performed a Bayesian meta-analysis, partially pooling the eight estimates toward a common mean. The model has the form, <span class="mathjax-tex">\(y_i\sim \mathrm{N}(\theta _i,\sigma ^2_i)\)</span> and <span class="mathjax-tex">\(\theta _i\sim \mathrm{N}(\mu ,\tau ^2)\)</span>, for <span class="mathjax-tex">\(i=1,\ldots ,n=8\)</span>, with a uniform prior distribution on <span class="mathjax-tex">\((\mu ,\tau )\)</span>. The measurements <span class="mathjax-tex">\(y_i\)</span> and uncertainties <span class="mathjax-tex">\(\sigma _i\)</span> are the estimates and standard errors from separate regressions performed for each school, as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s11222-016-9696-4#Tab1">1</a>. The test scores for the individual students are no longer available.</p><p>This model has eight parameters but they are constrained through their hierarchical distribution and are not estimated independently; thus we would anticipate the effective number of parameters should be some number between 1 and 8.</p><p>To better illustrate the behavior of LOO and WAIC, we repeat the analysis, rescaling the data points <i>y</i> by a factor ranging from 0.1 to 4 while keeping the standard errors <span class="mathjax-tex">\(\sigma \)</span> unchanged. With a small data scaling factor the hierarchical model nears complete pooling and with a large data scaling factor the model approaches separate fits to the data for each school. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig1">1</a> shows <span class="mathjax-tex">\(\widehat{\text{ elpd }}\)</span> for the various LOO approximation methods as a function of the scaling factor, based on 4000 simulation draws at each grid point.</p><p>When the data scaling factor is small (here, less than 1.5), all measures largely agree. As the data scaling factor increases and the model approaches no pooling, the population prior for <span class="mathjax-tex">\(\theta _i\)</span> gets flat and <span class="mathjax-tex">\(p_\mathrm{waic}\approx \frac{p}{2}\)</span>. This is correct behavior, as discussed by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. 24, 997–1016 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR13" id="ref-link-section-d71734e7546">2014</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Simulated 8 schools example: <b>a</b> Root mean square error of WAIC, Truncated Importance Sampling LOO, Importance Sampling LOO, Pareto Smoothed Importance Sampling LOO, and exact LOO with the true predictive performance computed using independently simulated test data; the error for all the methods increases, but the RMSE of exact LOO has an upper limit. Eventually the LOO approximations and WAIC fail to return exact LOO, as the leave-one-out posteriors are not close to the full posterior. When the estimated tail shape <i>k</i> exceeds 1, the importance-weighted LOO approximations start to fail. Among the approximations IS-LOO has the smallest RMSE as it has the smallest bias, and as the tail shape <i>k</i> is mostly below 1, it does not fail badly. <b>b</b> Root mean square error of WAIC, bias corrected Pareto Smoothed Importance Sampling LOO, and bias corrected exact LOO with the true predictive performance computed using independently simulated test data. The bias correction also reduces RMSE, having the clearest impact with smaller population distribution scales, but overall the reduction in RMSE is negligible. <b>c</b> Root mean square error of WAIC, Truncated Importance Sampling LOO with heavy truncation <span class="mathjax-tex">\((\root 4 \of {S}\bar{r})\)</span>, Pareto Smoothed Importance Sampling LOO, bias corrected exact LOO, and shrunk exact LOO with the true predictive performance computed using independently simulated test data. Truncated Importance Sampling LOO with heavy truncation matches WAIC accurately. Shrinking exact LOO towards the lpd of observed data reduces the RMSE for some scale values with small increase in error for larger scale values</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the case of exact LOO, <span class="mathjax-tex">\(\widehat{\mathrm{lpd}} - \widehat{\mathrm{elpd}}_\mathrm{loo}\)</span> can be larger than <i>p</i>. As the prior for <span class="mathjax-tex">\(\theta _i\)</span> approaches flatness, the log predictive density <span class="mathjax-tex">\(p_{\mathrm{post}(-i)}(y_i) \rightarrow -\infty \)</span>. At the same time, the full posterior becomes an inadequate approximation to <span class="mathjax-tex">\(p_{\mathrm{post}(-i)}\)</span> and all approximations become poor approximations to the actual out-of-sample prediction error under the model.</p><p>WAIC starts to fail when one of the posterior variances of the log predictive densities exceeds 0.4. LOO approximations work well even if the tail shape <i>k</i> of the generalized Pareto distribution is between <span class="mathjax-tex">\(\frac{1}{2}\)</span> and 1, and the variance of the raw importance ratios is infinite. The error of LOO approximations increases with <i>k</i>, with a clearer difference between the methods when <span class="mathjax-tex">\(k&gt;0.7\)</span>.</p><h3 class="c-article__sub-heading" id="Sec13">Example: Simulated 8 schools</h3><p>In the previous example, we used exact LOO as the gold standard. In this section, we generate simulated data from the same statistical model and compare predictive performance on independent test data. Even when the number of observations <i>n</i> is fixed, as the scale of the population distribution increases we observe the effect of weak prior information in hierarchical models discussed in the previous section and by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. 24, 997–1016 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR13" id="ref-link-section-d71734e7880">2014</a>). Comparing the error, bias and variance of the various approximations, we find that PSIS-LOO offers the best balance.</p><p>For <span class="mathjax-tex">\(i=1,\ldots ,n=8\)</span>, we simulate <span class="mathjax-tex">\(\theta _{0,i}\sim \mathrm{N}(\mu _0,\tau ^2_0)\)</span> and <span class="mathjax-tex">\(y_i\sim \mathrm{N}(\theta _{0,i},\sigma ^2_{0,i})\)</span>, where we set <span class="mathjax-tex">\(\sigma _{0,i}=10,\mu _0=0\)</span>, and <span class="mathjax-tex">\(\tau _0\in \{1,2,\ldots ,30\}\)</span>. The simulated data is similar to the real 8 schools data, for which the empirical estimate is <span class="mathjax-tex">\(\hat{\tau }\approx 10\)</span>. For each value of <span class="mathjax-tex">\(\tau _0\)</span> we generate 100 training sets of size 8 and one test data set of size 1000. Posterior inference is based on 4000 draws for each constructed model.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig2">2</a>a shows the root mean square error (RMSE) for the various LOO approximation methods as a function of <span class="mathjax-tex">\(\tau _0\)</span>, the scale of the population distribution. When <span class="mathjax-tex">\(\tau _0\)</span> is large all of the approximations eventually have ever increasing RMSE, while exact LOO has an upper limit. For medium scales the approximations have <i>smaller</i> RMSE than exact LOO. As discussed later, this is explained by the difference in the variance of the estimates. For small scales WAIC has slightly smaller RMSE than the other methods (including exact LOO).</p><p>
Watanabe (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Watanabe, S.: Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. J. Mach. Learn. Res. 11, 3571–3594 (2010)" href="/article/10.1007/s11222-016-9696-4#ref-CR36" id="ref-link-section-d71734e8298">2010</a>) shows that WAIC gives an asymptotically unbiased estimate of the out-of-sample prediction error—this does <i>not</i> hold for hierarchical models with weak prior information as shown by Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. 24, 997–1016 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR13" id="ref-link-section-d71734e8304">2014</a>)—but exact LOO is slightly biased as the LOO posteriors use only <span class="mathjax-tex">\(n-1\)</span> observations. WAIC’s different behavior can be understood through the truncated Taylor series correction to the lpd, that is, not using the entire series will bias it towards lpd (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec7">2.2</a>). The bias in LOO is negligible when <i>n</i> is large, but with small <i>n</i> it can be be larger.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig2">2</a>b shows RMSE for the bias corrected LOO approximations using the first order correction of Burman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Burman, P.: A comparative study of ordinary cross-validation, &#xA;                    &#xA;                      &#xA;                    &#xA;                    $$v$$&#xA;                    &#xA;                      &#xA;                        v&#xA;                      &#xA;                    &#xA;                  -fold cross-validation and the repeated learning-testing methods. Biometrika 76, 503–514 (1989)" href="/article/10.1007/s11222-016-9696-4#ref-CR5" id="ref-link-section-d71734e8350">1989</a>). For small scales the error of bias corrected LOOs is smaller than WAIC. When the scale increases the RMSEs are close to the non-corrected versions. Although the bias correction is easy to compute, the difference in accuracy is negligible for most applications.</p><p>We shall discuss Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig2">2</a>c in a moment, but first consider Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig3">3</a>, which shows the RMSE of the approximation methods and the lpd of observed data decomposed into bias and standard deviation. All methods (except the lpd of observed data) have small biases and variances with small population distribution scales. Bias corrected exact LOO has practically zero bias for all scale values but the highest variance. When the scale increases the LOO approximations eventually fail and bias increases. As the approximations start to fail, there is a certain region where implicit shrinkage towards the lpd of observed data decelerates the increase in RMSE as the variance is reduced, even if the bias continues to grow.</p><p>If the goal were to minimize the RMSE for smaller and medium scales, we could also shrink exact LOO and increase shrinkage in approximations. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig2">2</a>c shows the RMSE of the LOO approximations with two new choices. Truncated Importance Sampling LOO with very heavy truncation (to <span class="mathjax-tex">\(\root 4 \of {S}\bar{r}\)</span>) closely matches the performance of WAIC. In the experiments not included here, we also observed that adding more correct Taylor series terms to WAIC will make it behave similar to Truncated Importance Sampling with less truncation (see discussion of Taylor series expansion in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec7">2.2</a>). Shrunk exact LOO (<span class="mathjax-tex">\(\alpha \cdot \mathrm{elpd}_\mathrm{loo} + (1-\alpha )\cdot \text{ lpd }\)</span>, with <span class="mathjax-tex">\(\alpha =0.85\)</span> chosen by hand for illustrative purposes only) has a smaller RMSE for small and medium scale values as the variance is reduced, but the price is increased bias at larger scale values.</p><p>If the goal is robust estimation of predictive performance, then exact LOO is the best general choice because the error is limited even in the case of weak priors. Of the approximations, PSIS-LOO offers the best balance as well as diagnostics for identifying when it is likely failing.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Simulated 8 schools example: <b>a</b> Absolute bias of WAIC, Pareto Smoothed Importance Sampling LOO, bias corrected exact LOO, and the lpd (log predictive density) of observed data with the true predictive performance computed using independently simulated test data; <b>b</b> standard deviation for each of these measures; All methods except the lpd of observed data have small biases and variances with small population distribution scales. When the scale increases the bias of WAIC increases faster than the bias of the other methods (except the lpd of observed data). Bias corrected exact LOO has practically zero bias for all scale values. WAIC and Pareto Smoothed Importance Sampling LOO have lower variance than exact LOO, as they are shrunk towards the lpd of observed data, which has the smallest variance with all scales</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec14">Example: Linear regression for stack loss data</h3><p>To check the performance of the proposed diagnostic for our second example we analyze the stack loss data used by Peruggia (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J. Am. Stat. Assoc. 92, 199–207 (1997)" href="/article/10.1007/s11222-016-9696-4#ref-CR19" id="ref-link-section-d71734e8536">1997</a>) which is known to have analytically proven infinite variance of one of the importance weight distributions.</p><p>The data consist of <span class="mathjax-tex">\(n = 21\)</span> daily observations on one outcome and three predictors pertaining to a plant for the oxidation of ammonia to nitric acid. The outcome <i>y</i> is an inverse measure of the efficiency of the plant and the three predictors <span class="mathjax-tex">\(x_1, x_2\)</span>, and <span class="mathjax-tex">\(x_3\)</span> measure rate of operation, temperature of cooling water, and (a transformation of the) concentration of circulating acid.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Stack loss example with normal errors: Distributions of <b>a</b> tail shape estimates and <b>b</b> PSIS-LOO estimation errors compared to LOO, from 100 independent Stan runs. The pointwise calculation of the terms in PSIS-LOO reveals that much of the uncertainty comes from a single data point, and it could make sense to simply re-fit the model to the subset and compute LOO directly for that point</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>
Peruggia (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J. Am. Stat. Assoc. 92, 199–207 (1997)" href="/article/10.1007/s11222-016-9696-4#ref-CR19" id="ref-link-section-d71734e8659">1997</a>) shows that the importance weights for leave-one-out cross-validation for the data point <span class="mathjax-tex">\(y_{21}\)</span> have infinite variance. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig4">4</a> shows the distribution of the estimated tail shapes <span class="mathjax-tex">\(\hat{k}\)</span> and estimation errors compared to LOO in 100 independent Stan runs.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> The estimates of the tail shape <span class="mathjax-tex">\(\hat{k}\)</span> for <span class="mathjax-tex">\(i=21\)</span> suggest that the variance of the raw importance ratios is infinite, however the generalized central limit theorem for stable distributions holds and we can still obtain an accurate estimate of the component of LOO for this data point using PSIS.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Stack loss example with normal errors: <b>a</b> Tail shape estimate and <b>b</b> LOO approximations for the difficult point, <span class="mathjax-tex">\(i=21\)</span>. When more draws are obtained, the estimates converge (slowly) following the generalized central limit theorem</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig5">5</a> shows that if we continue sampling, the estimates for both the tail shape <span class="mathjax-tex">\(\hat{k}\)</span> and <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_i\)</span> do converge (although slowly as <span class="mathjax-tex">\(\hat{k}\)</span> is close to 1). As the convergence is slow it would be more efficient to sample directly from <span class="mathjax-tex">\(p(\theta ^s|y_{-i})\)</span> for the problematic <i>i</i>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Stack loss example with Student-<i>t</i> errors: Distributions of <b>a</b> tail shape estimates and <b>b</b> PSIS-LOO estimation errors compared to LOO, from 100 independent Stan runs. The computations are more stable than with normal errors (compare to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig4">4</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Puromycin example: Distributions of <b>a</b> tail shape estimates and <b>b</b> PSIS-LOO estimation errors compared to LOO, from 100 independent Stan runs. In an applied example we would only perform these calculations once, but here we replicate 100 times to give a sense of the Monte Carlo error of our procedure</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>High estimates of the tail shape parameter <span class="mathjax-tex">\(\hat{k}\)</span> indicate that the full posterior is not a good importance sampling approximation to the desired leave-one-out posterior, and thus the observation is surprising according to the model. It is natural to consider an alternative model. We tried replacing the normal observation model with a Student-<i>t</i> to make the model more robust for the possible outlier. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig6">6</a> shows the distribution of the estimated tail shapes <span class="mathjax-tex">\({\hat{k}}\)</span> and estimation errors for PSIS-LOO compared to LOO in 100 independent Stan runs for the Student-<i>t</i> linear regression model. The estimated tail shapes and the errors in computing this component of LOO are smaller than with Gaussian model.</p><h3 class="c-article__sub-heading" id="Sec15">Example: Nonlinear regression for Puromycin reaction data</h3><p>As a nonlinear regression example, we use the Puromycin biochemical reaction data also analyzed by Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e9097">2008</a>). For a group of cells not treated with the drug Puromycin, there are <span class="mathjax-tex">\(n = 11\)</span> measurements of the initial velocity of a reaction, <span class="mathjax-tex">\(V_i\)</span> , obtained when the concentration of the substrate was set at a given positive value, <span class="mathjax-tex">\(c_i\)</span>. Velocity on concentration is given by the Michaelis-Menten relation, <span class="mathjax-tex">\(V_i \sim \text{ N }(mc_i/(\kappa + c_i), \sigma ^2)\)</span>. Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e9264">2008</a>) show that the raw importance ratios for observation <span class="mathjax-tex">\(i=1\)</span> have infinite variance.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig7">7</a> shows the distribution of the estimated tail shapes <i>k</i> and estimation errors compared to LOO in 100 independent Stan runs. The estimates of the tail shape <i>k</i> for <span class="mathjax-tex">\(i=1\)</span> suggest that the variance of the raw importance ratios is infinite. However, the generalized central limit theorem for stable distributions still holds and we can get an accurate estimate of the corresponding term in LOO. We could obtain more draws to reduce the Monte Carlo error, or again consider a more robust model.</p><h3 class="c-article__sub-heading" id="Sec16">Example: Logistic regression for leukemia survival</h3><p>Our next example uses a logistic regression model to predict survival of leukemia patients past 50 weeks from diagnosis. These data were also analyzed by Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e9339">2008</a>). Explanatory variables are white blood cell count at diagnosis and whether “Auer rods and/or significant granulature of the leukemic cells in the bone marrow at diagnosis” were present.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Leukemia example: Distributions of <b>a</b> tail shape estimates and <b>b</b> PSIS-LOO estimation errors compared to LOO, from 100 independent Stan runs. The pointwise calculation of the terms in PSIS-LOO reveals that much of the uncertainty comes from a single data point, and it could make sense to simply re-fit the model to the subset and compute LOO directly for that point</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Leukemia example: Distributions of <b>a</b> tail shape estimate and <b>b</b> LOO approximations for <span class="mathjax-tex">\(i=15\)</span>. If we continue sampling, the tail shape estimate stays above 1 and <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_i\)</span> will not converge</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Leukemia example with log-transformed predictor: <b>a</b> Distributions of tail shape estimates for each data point and <b>b</b> PSIS-LOO estimation errors compared to LOO, from 100 independent Stan runs. Computations are more stable compared to the model fit on the original scale and displayed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig8">8</a>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Radon example: <b>a</b> Tail shape estimates for each point’s contribution to LOO, and <b>b</b> error in PSIS-LOO accuracy for each data point, all based on a single fit of the model in Stan</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>
Epifani et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. 2, 774–806 (2008)" href="/article/10.1007/s11222-016-9696-4#ref-CR6" id="ref-link-section-d71734e9505">2008</a>) show that the raw importance ratios for data point <span class="mathjax-tex">\(i=15\)</span> have infinite variance. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig8">8</a> shows the distribution of the estimated tail shapes <i>k</i> and estimation errors compared to LOO in 100 independent Stan runs. The estimates of the tail shape <i>k</i> for <span class="mathjax-tex">\(i=15\)</span> suggest that the mean and variance of the raw importance ratios do not exist. Thus the generalized central limit theorem does not hold.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig9">9</a> shows that if we continue sampling, the tail shape estimate stays above 1 and <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_i\)</span> will not converge.</p><p>Large estimates for the tail shape parameter indicate that the full posterior is not a good importance sampling approximation for the desired leave-one-out posterior, and thus the observation is surprising. The original model used the white blood cell count directly as a predictor, and it would be natural to use its logarithm instead. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig10">10</a> shows the distribution of the estimated tail shapes <i>k</i> and estimation errors compared to LOO in 100 independent Stan runs for this modified model. Both the tail shape values and errors are now smaller.</p><h3 class="c-article__sub-heading" id="Sec17">Example: Multilevel regression for radon contamination</h3><p>
Gelman and Hill (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Gelman, A., Hill, J.: Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press, Cambridge (2007)" href="/article/10.1007/s11222-016-9696-4#ref-CR12" id="ref-link-section-d71734e9624">2007</a>) describe a study conducted by the United States Environmental Protection Agency designed to measure levels of the carcinogen radon in houses throughout the United States. In high concentrations radon is known to cause lung cancer and is estimated to be responsible for several thousands of deaths every year in the United States. Here we focus on the sample of 919 houses in the state of Minnesota, which are distributed (unevenly) throughout 85 counties.</p><p>We fit the following multilevel linear model to the radon data</p><div id="Equ26" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} y_i&amp;\sim \mathrm{N}\left( \alpha _{j[i]} + \beta _{j[i]} x_i, \sigma ^2\right) , \quad i = 1, \ldots , 919 \\ \begin{pmatrix} \alpha _j \\ \beta _j \end{pmatrix}&amp;\sim \mathrm{N} \left( \begin{pmatrix} \gamma _0^\alpha + \gamma _1^\alpha u_j \\ \gamma _0^\beta + \gamma _1^\beta u_j \end{pmatrix}, \begin{pmatrix} \sigma ^2_\alpha &amp;{} \rho \sigma _\alpha \sigma _\beta \\ \rho \sigma _\alpha \sigma _\beta &amp;{} \sigma ^2_\beta \end{pmatrix} \right) ,\\&amp;j = 1, \ldots , 85, \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(y_i\)</span> is the logarithm of the radon measurement in the <i>i</i>th house, <span class="mathjax-tex">\(x_i = 0\)</span> for a measurement made in the basement and <span class="mathjax-tex">\(x_i = 1\)</span> if on the first floor (it is known that radon enters more easily when a house is built into the ground), and the county-level predictor <span class="mathjax-tex">\(u_j\)</span> is the logarithm of the soil uranium level in the county. The residual standard deviation <span class="mathjax-tex">\(\sigma \)</span> and all hyperparameters are given weakly informative priors. Code for fitting this model is provided in Appendix 3.</p><p>The sample size in this example <span class="mathjax-tex">\((n=919)\)</span> is not huge but is large enough that it is important to have a computational method for LOO that is fast for each data point. Although the MCMC for the full posterior inference (using four parallel chains) finished in only 93 s, the computations for exact brute force LOO require fitting the model 919 times and took more than 20 h to complete (Macbook Pro, 2.6 GHz Intel Core i7). With the same hardware the PSIS-LOO computations took less than 5 s.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig11">11</a> shows the results for the radon example and indeed the estimated shape parameters <i>k</i> are small and all of the tested methods are accurate. For two observations the estimate of <i>k</i> is slightly higher than the preferred threshold of 0.7, but we can easily compute the elpd contributions for these points directly and then combine with the PSIS-LOO estimates for the remaining observations.<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> This is the procedure we refer to as PSIS-LOO+ in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec18">4.7</a> below.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Root mean square error for different computations of LOO as determined from a simulation study, in each case based on running Stan to obtain 4000 posterior draws and repeating 100 times</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="/article/10.1007/s11222-016-9696-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Partial replication of Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s11222-016-9696-4#Tab2">2</a> using 16,000 posterior draws in each case</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="/article/10.1007/s11222-016-9696-4/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec18">Summary of examples</h3><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s11222-016-9696-4#Tab2">2</a> compares the performance of Pareto smoothed importance sampling, raw importance sampling, truncated importance sampling, and WAIC for estimating expected out-of-sample prediction accuracy for each of the examples in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec12">4.1</a>–<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec17">4.6</a>. Models were fit in Stan to obtain 4000 simulation draws. In each case, the distributions come from 100 independent simulations of the entire fitting process, and the root mean squared error is evaluated by comparing to exact LOO, which was computed by separately fitting the model to each leave-one-out dataset for each example. The last three lines of Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s11222-016-9696-4#Tab2">2</a> show additionally the performance of PSIS-LOO combined with direct sampling for the problematic <i>i</i> with <span class="mathjax-tex">\({\hat{k}}&gt;0.7\)</span> (PSIS-LOO+), 10-fold-CV, and 10 times repeated 10-fold-CV.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> For the Stacks-<i>N</i>, Puromycin, and Leukemia examples, there was one <i>i</i> with <span class="mathjax-tex">\({\hat{k}}&gt;0.7\)</span>, and thus the improvement has the same computational cost as the full posterior inference. 10-fold-CV has higher RMSE than LOO approximations except in the Leukemia case. The higher RMSE of 10-fold-CV is due to additional variance from the data division. The repeated 10-fold-CV has smaller RMSE than basic 10-fold-CV, but now the cost of computation is already 100 times the original full posterior inference. These results show that <i>K</i>-fold-CV is needed only if LOO approximations fail badly (see also the results in Vehtari and Lampinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e11170">2002</a>).</p><p>As measured by root mean squared error, PSIS consistently performs well. In general, when IS-LOO has problems it is because of the high variance of the raw importance weights, while TIS-LOO and WAIC have problems because of bias. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s11222-016-9696-4#Tab3">3</a> shows a replication using 16,000 Stan draws for each example. The results are similar results and PSIS-LOO is able to improve the most given additional draws.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Standard errors and model comparison</h2><div class="c-article-section__content" id="Sec19-content"><p>We next consider some approaches for assessing the uncertainty of cross-validation and WAIC estimates of prediction error. We present these methods in a separate section rather than in our main development because, as discussed below, the diagnostics can be difficult to interpret when the sample size is small.</p><h3 class="c-article__sub-heading" id="Sec20">Standard errors</h3><p>The computed estimates <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo}\)</span> and <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{waic}\)</span> are each defined as the sum of <i>n</i> independent components so it is trivial to compute their standard errors by computing the standard deviation of the <i>n</i> components and multiplying by <span class="mathjax-tex">\(\sqrt{n}\)</span>. For example, define</p><div id="Equ22" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ22_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (22)
                </div></div><p>so that <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo}\)</span> in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ4">4</a>) is the sum of these <i>n</i> independent terms. Then</p><div id="Equ23" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Equ23_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (23)
                </div></div><p>and similarly for WAIC and <i>K</i>-fold cross-validation. The effective numbers of parameters, <span class="mathjax-tex">\(\widehat{p}_\mathrm{loo}\)</span> and <span class="mathjax-tex">\( \widehat{p}_\mathrm{waic}\)</span>, are also sums of independent terms so we can compute their standard errors in the same way.</p><p>These standard errors come from considering the <i>n</i> data points as a sample from a larger population or, equivalently, as independent realizations of an error model. One can also compute Monte Carlo standard errors arising from the finite number of simulation draws using the formula from Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.: Bayesian Data Analysis, 3rd edn. CRC Press, London (2013)" href="/article/10.1007/s11222-016-9696-4#ref-CR11" id="ref-link-section-d71734e11418">2013</a>) which uses both between and within-chain information and is implemented in Stan. In practice we expect Monte Carlo standard errors to not be so interesting because we would hope to have enough simulations that the computations are stable, but it could make sense to look at them just to check that they are low enough to be negligible compared to sampling error (which scales like 1 / <i>n</i> rather than 1 / <i>S</i>).</p><p>The standard error (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ23">23</a>) and the corresponding formula for <span class="mathjax-tex">\(\text{ se }\,(\widehat{\mathrm{elpd}}_\mathrm{waic})\)</span> have two difficulties when the sample size is low. First, the <i>n</i> terms are not strictly independent because they are all computed from the same set of posterior simulations <span class="mathjax-tex">\(\theta ^s\)</span>. This is a generic issue when evaluating the standard error of any cross-validated estimate. Second, the terms in any of these expressions can come from highly skewed distributions, so the second moment might not give a good summary of uncertainty. Both of these problems should subside as <i>n</i> becomes large. For small <i>n</i>, one could instead compute nonparametric error estimates using a Bayesian bootstrap on the computed log-likelihood values corresponding to the <i>n</i> data points (Vehtari and Lampinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e11521">2002</a>).</p><h3 class="c-article__sub-heading" id="Sec21">Model comparison</h3><p>When comparing two fitted models, we can estimate the difference in their expected predictive accuracy by the difference in <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo}\)</span> or <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{waic}\)</span> (multiplied by <span class="mathjax-tex">\(-2\)</span>, if desired, to be on the deviance scale). To compute the standard error of this difference we can use a paired estimate to take advantage of the fact that the same set of <i>n</i> data points is being used to fit both models.</p><p>For example, suppose we are comparing models A and B, with corresponding fit measures <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo}^A=\sum _{i=1}^n \widehat{\mathrm{elpd}}_{\mathrm{loo},i}^A\)</span> and <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo}^B=\sum _{i=1}^n \widehat{\mathrm{elpd}}_{\mathrm{loo},i}^B\)</span>. The standard error of their difference is simply,</p><div id="Equ24" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \text{ se }\,(\widehat{\mathrm{elpd}}_\mathrm{loo}^A- \widehat{\mathrm{elpd}}_\mathrm{loo}^B) =\sqrt{n\,V_{i=1}^n (\widehat{\mathrm{elpd}}_{\mathrm{loo},i}^A- \widehat{\mathrm{elpd}}_{\mathrm{loo},i}^B)},\nonumber \\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (24)
                </div></div><p>and similarly for WAIC and <i>K</i>-fold cross-validation. Alternatively the non-parametric Bayesian bootstrap approach can be used (Vehtari and Lampinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. 14, 2439–2468 (2002)" href="/article/10.1007/s11222-016-9696-4#ref-CR33" id="ref-link-section-d71734e11984">2002</a>).</p><p>As before, these calculations should be most useful when <i>n</i> is large, because then non-normality of the distribution is not such an issue when estimating the uncertainty of these sums.</p><p>In any case, we suspect that these standard error formulas, for all their flaws, should give a better sense of uncertainty than what is obtained using the current standard approach for comparing differences of deviances to a <span class="mathjax-tex">\(\chi ^2\)</span> distribution, a practice that is derived for Gaussian linear models or asymptotically and, in any case, only applies to nested models.</p><p>Further research needs to be done to evaluate the performance in model comparison of (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ24">24</a>) and the corresponding standard error formula for LOO. Cross-validation and WAIC should not be used to select a single model among a large number of models due to a selection induced bias as demonstrated, for example, by Piironen and Vehtari (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Piironen, J., Vehtari, A.: Comparison of Bayesian predictive methods for model selection. Stat. Comput. (2016) (In press). &#xA;                    http://link.springer.com/article/10.1007/s11222-016-9649-y&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s11222-016-9696-4#ref-CR20" id="ref-link-section-d71734e12028">2016</a>).</p><p>We demonstrate the practical use of LOO in model comparison using the radon example from Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec17">4.6</a>. Model A is the multilevel linear model discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec17">4.6</a> and Model B is the same model but without the county-level uranium predictor. That is, at the county-level Model B has</p><div id="Equ27" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \begin{pmatrix} \alpha _j \\ \beta _j \end{pmatrix} \sim \mathrm{N} \left( \begin{pmatrix} \mu _\alpha \\ \mu _\beta \end{pmatrix}, \begin{pmatrix} \sigma ^2_\alpha &amp;{} \rho \sigma _\alpha \sigma _\beta \\ \rho \sigma _\alpha \sigma _\beta &amp;{} \sigma ^2_\beta \end{pmatrix} \right) , \quad j = 1, \ldots , 85. \end{aligned}$$</span></div></div><p>Comparing the models on PSIS-LOO reveals an estimated difference in elpd of 10.2 (with a standard error of 5.1) in favor of Model A.</p><h3 class="c-article__sub-heading" id="Sec22">Model comparison using pointwise prediction errors</h3><p>We can also compare models in their leave-one-out errors, point by point. We illustrate with an analysis of a survey of residents from a small area in Bangladesh that was affected by arsenic in drinking water. Respondents with elevated arsenic levels in their wells were asked if they were interested in getting water from a neighbor’s well, and a series of models were fit to predict this binary response given various information about the households (Gelman and Hill <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Gelman, A., Hill, J.: Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press, Cambridge (2007)" href="/article/10.1007/s11222-016-9696-4#ref-CR12" id="ref-link-section-d71734e12304">2007</a>).</p><p>Here we start with a logistic regression for the well-switching response given two predictors: the arsenic level of the water in the resident’s home, and the distance of the house from the nearest safe well. We compare this to an alternative logistic regression with the arsenic predictor on the logarithmic scale. The two models have the same number of parameters but give different predictions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Arsenic example, comparing two models in terms of their pointwise contributions to LOO: <b>a</b> comparing contributions of LOO directly; <b>b</b> plotting the difference in LOO as a function of a key predictor (the existing arsenic level). To aid insight, we have colored the data according to the (binary) output, with red corresponding to <span class="mathjax-tex">\(y=1\)</span> and blue representing <span class="mathjax-tex">\(y=0\)</span>. For any given data point, one model will fit better than another, but for this example the graphs reveal that the difference in LOO between the models arises from the linear model’s poor predictions for 10–15 non-switchers with high arsenic levels</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s11222-016-9696-4/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig12">12</a> shows the pointwise results for the arsenic example. The scattered blue dots on the left side of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig12">12</a>a and on the lower right of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig12">12</a>b correspond to data points which Model A fits particularly poorly—that is, large negative contributions to the expected log predictive density. We can also sum these <i>n</i> terms to yield an estimated difference in <span class="mathjax-tex">\(\mathrm{elpd}_\mathrm{loo}\)</span> of 16.4 with a standard error of 4.4. This standard error derives from the finite sample size and is scaled by the variation in the differences displayed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s11222-016-9696-4#Fig12">12</a>; it is <i>not</i> a Monte Carlo error and does not decline to 0 as the number of Stan simulation draws increases.</p></div></div></section><section aria-labelledby="Sec23"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23">Discussion</h2><div class="c-article-section__content" id="Sec23-content"><p>This paper has focused on the practicalities of implementing LOO, WAIC, and <i>K</i>-fold cross-validation within a Bayesian simulation environment, in particular the coding of the log-likelihood in the model, the computations of the information measures, and the stabilization of weights to enable an approximation of LOO without requiring refitting the model.</p><p>Some difficulties persist, however. As discussed above, any predictive accuracy measure involves two definitions: (1) the choice of what part of the model to label as “the likelihood”, which is directly connected to which potential replications are being considered for out-of-sample prediction; and (2) the factorization of the likelihood into “data points”, which is reflected in the later calculations of expected log predictive density.</p><p>Some choices of replication can seem natural for a particular dataset but less so in other comparable settings. For example, the 8 schools data are available only at the school level and so it seems natural to treat the school-level estimates as data. But if the original data had been available, we would surely have defined the likelihood based on the individual students’ test scores. It is an awkward feature of predictive error measures that they might be determined based on computational convenience or data availability rather than fundamental features of the problem. To put it another way, we are assessing the fit of the model to the particular data at hand.</p><p>Finally, these methods all have limitations. The concern with WAIC is that formula (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s11222-016-9696-4#Equ12">12</a>) is an asymptotic expression for the bias of lpd for estimating out-of-sample prediction error and is only an approximation for finite samples. Cross-validation (whether calculated directly by re-fitting the model to several different data subsets, or approximated using importance sampling as we did for LOO) has a different problem in that it relies on inference from a smaller subset of the data being close to inference from the full dataset, an assumption that is typically but not always true.</p><p>For example, as we demonstrated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec12">4.1</a>, in a hierarchical model with only one data point per group, PSIS-LOO and WAIC can dramatically understate prediction accuracy. Another setting where LOO (and cross-validation more generally) can fail is in models with weak priors and sparse data. For example, consider logistic regression with flat priors on the coefficients and data that happen to be so close to separation that the removal of a single data point can induce separation and thus infinite parameter estimates. In this case the LOO estimate of average prediction accuracy will be zero (that is, <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{is-loo}\)</span> will be <span class="mathjax-tex">\(-\infty \)</span>) if it is calculated to full precision, even though predictions of future data from the actual fitted model will have bounded loss. Such problems should not arise asymptotically with a fixed model and increasing sample size but can occur with actual finite data, especially in settings where models are increasing in complexity and are insufficiently constrained.</p><p>That said, quick estimates of out-of-sample prediction error can be valuable for summarizing and comparing models, as can be seen from the popularity of AIC and DIC. For Bayesian models, we prefer PSIS-LOO and K-fold cross-validation to those approximations which are based on point estimation.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>The <span class="u-monospace">loo</span> R package is available from CRAN and <a href="https://github.com/stan-dev/loo">https://github.com/stan-dev/loo</a>. The corresponding code for Matlab, Octave, and Python is available at <a href="https://github.com/avehtari/PSIS">https://github.com/avehtari/PSIS</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>In Gelman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.: Bayesian Data Analysis, 3rd edn. CRC Press, London (2013)" href="/article/10.1007/s11222-016-9696-4#ref-CR11" id="ref-link-section-d71734e3900">2013</a>), the variance-based <span class="mathjax-tex">\(p_\mathrm{waic}\)</span> defined here is called <span class="mathjax-tex">\(p_{\mathrm{waic}\, 2}\)</span>. There is also a mean-based formula, <span class="mathjax-tex">\(p_{\mathrm{waic}\, 1}\)</span>, which we do not use here.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>Smoothed density estimates were made using a logistic Gaussian process (Vehtari and Riihimäki <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Vehtari, A., Riihimäki, J.: Laplace approximation for logistic Gaussian process density estimation and regression. Bayesian Anal. 9, 425–448 (2014)" href="/article/10.1007/s11222-016-9696-4#ref-CR35" id="ref-link-section-d71734e8718">2014</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>As expected, the two slightly high estimates for <i>k</i> correspond to particularly influential observations, in this case houses with extremely low radon measurements.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>10-fold-CV results were not computed for data sets with <span class="mathjax-tex">\(n\le 11\)</span>, and 10 times repeated 10-fold-CV was not feasible for the radon example due to the computation time required.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>The code in the generated quantities block is written using the new syntax introduced in Stan version 2.10.0.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>For models fit to large datasets it can be infeasible to store the entire log-likelihood matrix in memory. A function for computing the log-likelihood from the data and posterior draws of the relevant parameters may be specified instead of the log-likelihood matrix—the necessary data and draws are supplied as an additional argument—and columns of the log-likelihood matrix are computed as needed. This requires less memory than storing the entire log-likelihood matrix and allows <span class="u-monospace">loo</span> to be used with much larger datasets.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8"><span class="c-article-footnote--listed__index">8.</span><div class="c-article-footnote--listed__content"><p>In statistics there is a tradition of looking at deviance, while in computer science the log score is more popular, so we return both.</p></div></li><li class="c-article-footnote--listed__item" id="Fn9"><span class="c-article-footnote--listed__index">9.</span><div class="c-article-footnote--listed__content"><p>The <span class="u-monospace">extract_log_lik</span>() function used in the example is a convenience function for extracting the log-likelihood matrix from a fitted Stan model provided that the user has computed and stored the pointwise log-likelihood in their Stan program (see, for example, the <span class="u-monospace">generated quantities</span> block in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec25">1</a>). The argument <span class="u-monospace">parameter_name</span> (defaulting to <span class="u-monospace">“log_lik”</span>) can also be supplied to indicate which parameter or generated quantity corresponds to the log-likelihood.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Akaike, H.: Information theory and an extension of the maximum likelihood principle. In: Petrov, B.N., Csaki, " /><p class="c-article-references__text" id="ref-CR1">Akaike, H.: Information theory and an extension of the maximum likelihood principle. In: Petrov, B.N., Csaki, F. (eds.) Proceedings of the Second International Symposium on Information Theory, pp. 267–281. Akademiai Kiado, Budapest (1973)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Ando, R. Tsay, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Ando, T., Tsay, R.: Predictive likelihood for Bayesian model selection and averaging. Int. J. Forecast. 26, 74" /><p class="c-article-references__text" id="ref-CR2">Ando, T., Tsay, R.: Predictive likelihood for Bayesian model selection and averaging. Int. J. Forecast. <b>26</b>, 744–763 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijforecast.2009.08.001" aria-label="View reference 2">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Predictive%20likelihood%20for%20Bayesian%20model%20selection%20and%20averaging&amp;journal=Int.%20J.%20Forecast.&amp;volume=26&amp;pages=744-763&amp;publication_year=2010&amp;author=Ando%2CT&amp;author=Tsay%2CR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Arolot, A. Celisse, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Arolot, S., Celisse, A.: A survey of cross-validation procedures for model selection. Stat. Surv. 4, 40–79 (20" /><p class="c-article-references__text" id="ref-CR3">Arolot, S., Celisse, A.: A survey of cross-validation procedures for model selection. Stat. Surv. <b>4</b>, 40–79 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2602303" aria-label="View reference 3 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1214%2F09-SS054" aria-label="View reference 3">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1190.62080" aria-label="View reference 3 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20cross-validation%20procedures%20for%20model%20selection&amp;journal=Stat.%20Surv.&amp;volume=4&amp;pages=40-79&amp;publication_year=2010&amp;author=Arolot%2CS&amp;author=Celisse%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bernardo, J.M., Smith A.F.M.: Bayesian Theory. Wiley, New York (1994)" /><p class="c-article-references__text" id="ref-CR4">Bernardo, J.M., Smith A.F.M.: Bayesian Theory. Wiley, New York (1994)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Burman, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Burman, P.: A comparative study of ordinary cross-validation, \(v\)-fold cross-validation and the repeated lea" /><p class="c-article-references__text" id="ref-CR5">Burman, P.: A comparative study of ordinary cross-validation, <span class="mathjax-tex">\(v\)</span>-fold cross-validation and the repeated learning-testing methods. Biometrika <b>76</b>, 503–514 (1989)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1040644" aria-label="View reference 5 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1093%2Fbiomet%2F76.3.503" aria-label="View reference 5">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0677.62065" aria-label="View reference 5 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20ordinary%20cross-validation%2C%20%24%24v%24%24%20v%20-fold%20cross-validation%20and%20the%20repeated%20learning-testing%20methods&amp;journal=Biometrika&amp;volume=76&amp;pages=503-514&amp;publication_year=1989&amp;author=Burman%2CP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Epifani, SN. MacEachern, M. Peruggia, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theor" /><p class="c-article-references__text" id="ref-CR6">Epifani, I., MacEachern, S.N., Peruggia, M.: Case-deletion importance sampling estimators: central limit theorems and related results. Electron. J. Stat. <b>2</b>, 774–806 (2008)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2443196" aria-label="View reference 6 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1214%2F08-EJS259" aria-label="View reference 6">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1320.62046" aria-label="View reference 6 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Case-deletion%20importance%20sampling%20estimators%3A%20central%20limit%20theorems%20and%20related%20results&amp;journal=Electron.%20J.%20Stat.&amp;volume=2&amp;pages=774-806&amp;publication_year=2008&amp;author=Epifani%2CI&amp;author=MacEachern%2CSN&amp;author=Peruggia%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gabry, J., Goodrich, B.: rstanarm: Bayesian applied regression modeling via Stan. R package version 2.10.0. (2" /><p class="c-article-references__text" id="ref-CR7">Gabry, J., Goodrich, B.: rstanarm: Bayesian applied regression modeling via Stan. R package version 2.10.0. (2016). <a href="http://mc-stan.org/interfaces/rstanarm">http://mc-stan.org/interfaces/rstanarm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Geisser, W. Eddy, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="Geisser, S., Eddy, W.: A predictive approach to model selection. J. Am. Stat. Assoc. 74, 153–160 (1979)" /><p class="c-article-references__text" id="ref-CR8">Geisser, S., Eddy, W.: A predictive approach to model selection. J. Am. Stat. Assoc. <b>74</b>, 153–160 (1979)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=529531" aria-label="View reference 8 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1080%2F01621459.1979.10481632" aria-label="View reference 8">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0401.62036" aria-label="View reference 8 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20predictive%20approach%20to%20model%20selection&amp;journal=J.%20Am.%20Stat.%20Assoc.&amp;volume=74&amp;pages=153-160&amp;publication_year=1979&amp;author=Geisser%2CS&amp;author=Eddy%2CW">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AE. Gelfand, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Gelfand, A.E.: Model determination using sampling-based methods. In: Gilks, W.R., Richardson, S., Spiegelhalte" /><p class="c-article-references__text" id="ref-CR9">Gelfand, A.E.: Model determination using sampling-based methods. In: Gilks, W.R., Richardson, S., Spiegelhalter, D.J. (eds.) Markov Chain Monte Carlo in Practice, pp. 145–162. Chapman and Hall, London (1996)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Markov%20Chain%20Monte%20Carlo%20in%20Practice&amp;pages=145-162&amp;publication_year=1996&amp;author=Gelfand%2CAE">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AE. Gelfand, DK. Dey, H. Chang, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Gelfand, A.E., Dey, D.K., Chang, H.: Model determination using predictive distributions with implementation vi" /><p class="c-article-references__text" id="ref-CR10">Gelfand, A.E., Dey, D.K., Chang, H.: Model determination using predictive distributions with implementation via sampling-based methods. In: Bernardo, J.M., Berger, J.O., Dawid, A.P., Smith, A.F.M. (eds.) Bayesian Statistics, 4th edn, pp. 147–167. Oxford University Press, Oxford (1992)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20Statistics&amp;pages=147-167&amp;publication_year=1992&amp;author=Gelfand%2CAE&amp;author=Dey%2CDK&amp;author=Chang%2CH">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Gelman, JB. Carlin, HS. Stern, DB. Dunson, A. Vehtari, DB. Rubin, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.: Bayesian Data Analysis, 3rd edn" /><p class="c-article-references__text" id="ref-CR11">Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.: Bayesian Data Analysis, 3rd edn. CRC Press, London (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20Data%20Analysis&amp;publication_year=2013&amp;author=Gelman%2CA&amp;author=Carlin%2CJB&amp;author=Stern%2CHS&amp;author=Dunson%2CDB&amp;author=Vehtari%2CA&amp;author=Rubin%2CDB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Gelman, J. Hill, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Gelman, A., Hill, J.: Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University " /><p class="c-article-references__text" id="ref-CR12">Gelman, A., Hill, J.: Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press, Cambridge (2007)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20Analysis%20Using%20Regression%20and%20Multilevel%2FHierarchical%20Models&amp;publication_year=2007&amp;author=Gelman%2CA&amp;author=Hill%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Gelman, J. Hwang, A. Vehtari, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. C" /><p class="c-article-references__text" id="ref-CR13">Gelman, A., Hwang, J., Vehtari, A.: Understanding predictive information criteria for Bayesian models. Stat. Comput. <b>24</b>, 997–1016 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3253850" aria-label="View reference 13 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs11222-013-9416-2" aria-label="View reference 13">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1332.62090" aria-label="View reference 13 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20predictive%20information%20criteria%20for%20Bayesian%20models&amp;journal=Stat.%20Comput.&amp;volume=24&amp;pages=997-1016&amp;publication_year=2014&amp;author=Gelman%2CA&amp;author=Hwang%2CJ&amp;author=Vehtari%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gneiting, T., Raftery, A.E.: Strictly proper scoring rules, prediction, and estimation. J. Am. Stat. Assoc. 10" /><p class="c-article-references__text" id="ref-CR14">Gneiting, T., Raftery, A.E.: Strictly proper scoring rules, prediction, and estimation. J. Am. Stat. Assoc. <b>102</b>, 359–378 (2007)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Hoeting, D. Madigan, AE. Raftery, C. Volinsky, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Hoeting, J., Madigan, D., Raftery, A.E., Volinsky, C.: Bayesian model averaging. Stat. Sci. 14, 382–417 (1999)" /><p class="c-article-references__text" id="ref-CR15">Hoeting, J., Madigan, D., Raftery, A.E., Volinsky, C.: Bayesian model averaging. Stat. Sci. <b>14</b>, 382–417 (1999)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1765176" aria-label="View reference 15 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1214%2Fss%2F1009212519" aria-label="View reference 15">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1059.62525" aria-label="View reference 15 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20model%20averaging&amp;journal=Stat.%20Sci.&amp;volume=14&amp;pages=382-417&amp;publication_year=1999&amp;author=Hoeting%2CJ&amp;author=Madigan%2CD&amp;author=Raftery%2CAE&amp;author=Volinsky%2CC">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MD. Hoffman, A. Gelman, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Hoffman, M.D., Gelman, A.: The no-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. " /><p class="c-article-references__text" id="ref-CR16">Hoffman, M.D., Gelman, A.: The no-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res. <b>15</b>, 1593–1623 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3214779" aria-label="View reference 16 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1319.60150" aria-label="View reference 16 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20no-U-turn%20sampler%3A%20adaptively%20setting%20path%20lengths%20in%20Hamiltonian%20Monte%20Carlo&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=15&amp;pages=1593-1623&amp;publication_year=2014&amp;author=Hoffman%2CMD&amp;author=Gelman%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EL. Ionides, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Ionides, E.L.: Truncated importance sampling. J. Comput. Graph. Stat. 17, 295–311 (2008)" /><p class="c-article-references__text" id="ref-CR17">Ionides, E.L.: Truncated importance sampling. J. Comput. Graph. Stat. <b>17</b>, 295–311 (2008)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2439961" aria-label="View reference 17 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1198%2F106186008X320456" aria-label="View reference 17">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Truncated%20importance%20sampling&amp;journal=J.%20Comput.%20Graph.%20Stat.&amp;volume=17&amp;pages=295-311&amp;publication_year=2008&amp;author=Ionides%2CEL">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Koopman, N. Shephard, D. Creal, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Koopman, S.J., Shephard, N., Creal, D.: Testing the assumptions behind importance sampling. J. Econom. 149, 2–" /><p class="c-article-references__text" id="ref-CR18">Koopman, S.J., Shephard, N., Creal, D.: Testing the assumptions behind importance sampling. J. Econom. <b>149</b>, 2–11 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2509092" aria-label="View reference 18 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.jeconom.2008.10.002" aria-label="View reference 18">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?06600647" aria-label="View reference 18 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Testing%20the%20assumptions%20behind%20importance%20sampling&amp;journal=J.%20Econom.&amp;volume=149&amp;pages=2-11&amp;publication_year=2009&amp;author=Koopman%2CSJ&amp;author=Shephard%2CN&amp;author=Creal%2CD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Peruggia, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J." /><p class="c-article-references__text" id="ref-CR19">Peruggia, M.: On the variability of case-deletion importance sampling weights in the Bayesian linear model. J. Am. Stat. Assoc. <b>92</b>, 199–207 (1997)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1436108" aria-label="View reference 19 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1080%2F01621459.1997.10473617" aria-label="View reference 19">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0889.62020" aria-label="View reference 19 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20variability%20of%20case-deletion%20importance%20sampling%20weights%20in%20the%20Bayesian%20linear%20model&amp;journal=J.%20Am.%20Stat.%20Assoc.&amp;volume=92&amp;pages=199-207&amp;publication_year=1997&amp;author=Peruggia%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Piironen, J., Vehtari, A.: Comparison of Bayesian predictive methods for model selection. Stat. Comput. (2016)" /><p class="c-article-references__text" id="ref-CR20">Piironen, J., Vehtari, A.: Comparison of Bayesian predictive methods for model selection. Stat. Comput. (2016) (<b>In press</b>). <a href="http://link.springer.com/article/10.1007/s11222-016-9649-y">http://link.springer.com/article/10.1007/s11222-016-9649-y</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Plummer, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Plummer, M.: Penalized loss functions for Bayesian model comparison. Biostatistics 9, 523–539 (2008)" /><p class="c-article-references__text" id="ref-CR21">Plummer, M.: Penalized loss functions for Bayesian model comparison. Biostatistics <b>9</b>, 523–539 (2008)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1093%2Fbiostatistics%2Fkxm049" aria-label="View reference 21">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1143.62003" aria-label="View reference 21 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Penalized%20loss%20functions%20for%20Bayesian%20model%20comparison&amp;journal=Biostatistics&amp;volume=9&amp;pages=523-539&amp;publication_year=2008&amp;author=Plummer%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="R Core Team: R: A language and environment for statistical computing. R Foundation for Statistical Computing, " /><p class="c-article-references__text" id="ref-CR22">R Core Team: R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria (2016). <a href="https://www.R-project.org/">https://www.R-project.org/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DB. Rubin, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Rubin, D.B.: Estimation in parallel randomized experiments. J. Educ. Stat. 6, 377–401 (1981)" /><p class="c-article-references__text" id="ref-CR23">Rubin, D.B.: Estimation in parallel randomized experiments. J. Educ. Stat. <b>6</b>, 377–401 (1981)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimation%20in%20parallel%20randomized%20experiments&amp;journal=J.%20Educ.%20Stat.&amp;volume=6&amp;pages=377-401&amp;publication_year=1981&amp;author=Rubin%2CDB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Spiegelhalter, NG. Best, BP. Carlin, A. Linde, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Spiegelhalter, D.J., Best, N.G., Carlin, B.P., van der Linde, A.: Bayesian measures of model complexity and fi" /><p class="c-article-references__text" id="ref-CR24">Spiegelhalter, D.J., Best, N.G., Carlin, B.P., van der Linde, A.: Bayesian measures of model complexity and fit. J. R. Stat. Soc. B <b>64</b>, 583–639 (2002)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1979380" aria-label="View reference 24 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1111%2F1467-9868.00353" aria-label="View reference 24">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1067.62010" aria-label="View reference 24 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20measures%20of%20model%20complexity%20and%20fit&amp;journal=J.%20R.%20Stat.%20Soc.%20B&amp;volume=64&amp;pages=583-639&amp;publication_year=2002&amp;author=Spiegelhalter%2CDJ&amp;author=Best%2CNG&amp;author=Carlin%2CBP&amp;author=Linde%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Spiegelhalter, D., Thomas, A., Best, N., Gilks, W., Lunn, D.: BUGS: Bayesian inference using Gibbs sampling. M" /><p class="c-article-references__text" id="ref-CR25">Spiegelhalter, D., Thomas, A., Best, N., Gilks, W., Lunn, D.: BUGS: Bayesian inference using Gibbs sampling. MRC Biostatistics Unit, Cambridge, England (1994, 2003). <a href="http://www.mrc-bsu.cam.ac.uk/bugs/">http://www.mrc-bsu.cam.ac.uk/bugs/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stan Development Team: The Stan C++ Library, version 2.10.0 (2016a). http://mc-stan.org/&#xA;                     " /><p class="c-article-references__text" id="ref-CR26">Stan Development Team: The Stan C++ Library, version 2.10.0 (2016a). <a href="http://mc-stan.org/">http://mc-stan.org/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stan Development Team: RStan: the R interface to Stan, version 2.10.1 (2016b). http://mc-stan.org/interfaces/r" /><p class="c-article-references__text" id="ref-CR27">Stan Development Team: RStan: the R interface to Stan, version 2.10.1 (2016b). <a href="http://mc-stan.org/interfaces/rstan.html">http://mc-stan.org/interfaces/rstan.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Stone, " /><meta itemprop="datePublished" content="1977" /><meta itemprop="headline" content="Stone, M.: An asymptotic equivalence of choice of model cross-validation and Akaike’s criterion. J. R. Stat. S" /><p class="c-article-references__text" id="ref-CR28">Stone, M.: An asymptotic equivalence of choice of model cross-validation and Akaike’s criterion. J. R. Stat. Soc. B <b>36</b>, 44–47 (1977)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=501454" aria-label="View reference 28 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0355.62002" aria-label="View reference 28 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20asymptotic%20equivalence%20of%20choice%20of%20model%20cross-validation%20and%20Akaike%E2%80%99s%20criterion&amp;journal=J.%20R.%20Stat.%20Soc.%20B&amp;volume=36&amp;pages=44-47&amp;publication_year=1977&amp;author=Stone%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Linde, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="van der Linde, A.: DIC in variable selection. Stat. Neerl. 1, 45–56 (2005)" /><p class="c-article-references__text" id="ref-CR29">van der Linde, A.: DIC in variable selection. Stat. Neerl. <b>1</b>, 45–56 (2005)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2142697" aria-label="View reference 29 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-9574.2005.00278.x" aria-label="View reference 29">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1069.62005" aria-label="View reference 29 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=DIC%20in%20variable%20selection&amp;journal=Stat.%20Neerl.&amp;volume=1&amp;pages=45-56&amp;publication_year=2005&amp;author=Linde%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vehtari, A., Gelman, A.: Pareto smoothed importance sampling (2015). arXiv:1507.02646&#xA;                        " /><p class="c-article-references__text" id="ref-CR30">Vehtari, A., Gelman, A.: Pareto smoothed importance sampling (2015). <a href="http://arxiv.org/abs/1507.02646">arXiv:1507.02646</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vehtari, A., Gelman, A., Gabry, J.: loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models" /><p class="c-article-references__text" id="ref-CR31">Vehtari, A., Gelman, A., Gabry, J.: loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models. R package version 0.1.6 (2016a). <a href="https://github.com/stan-dev/loo">https://github.com/stan-dev/loo</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vehtari, A., Mononen, T., Tolvanen, V., Sivula, T., Winther, O.: Bayesian leave-one-out cross-validation appro" /><p class="c-article-references__text" id="ref-CR32">Vehtari, A., Mononen, T., Tolvanen, V., Sivula, T., Winther, O.: Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. J. Mach. Learn. Res. <b>17</b>, 1–38 (2016b)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Vehtari, J. Lampinen, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densitie" /><p class="c-article-references__text" id="ref-CR33">Vehtari, A., Lampinen, J.: Bayesian model assessment and comparison using cross-validation predictive densities. Neural Comput. <b>14</b>, 2439–2468 (2002)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1162%2F08997660260293292" aria-label="View reference 33">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1002.62029" aria-label="View reference 33 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20model%20assessment%20and%20comparison%20using%20cross-validation%20predictive%20densities&amp;journal=Neural%20Comput.&amp;volume=14&amp;pages=2439-2468&amp;publication_year=2002&amp;author=Vehtari%2CA&amp;author=Lampinen%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Vehtari, J. Ojanen, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Vehtari, A., Ojanen, J.: A survey of Bayesian predictive methods for model assessment, selection and compariso" /><p class="c-article-references__text" id="ref-CR34">Vehtari, A., Ojanen, J.: A survey of Bayesian predictive methods for model assessment, selection and comparison. Stat. Surv. <b>6</b>, 142–228 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3011074" aria-label="View reference 34 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1214%2F12-SS102" aria-label="View reference 34">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1302.62011" aria-label="View reference 34 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20Bayesian%20predictive%20methods%20for%20model%20assessment%2C%20selection%20and%20comparison&amp;journal=Stat.%20Surv.&amp;volume=6&amp;pages=142-228&amp;publication_year=2012&amp;author=Vehtari%2CA&amp;author=Ojanen%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Vehtari, J. Riihimäki, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Vehtari, A., Riihimäki, J.: Laplace approximation for logistic Gaussian process density estimation and regress" /><p class="c-article-references__text" id="ref-CR35">Vehtari, A., Riihimäki, J.: Laplace approximation for logistic Gaussian process density estimation and regression. Bayesian Anal. <b>9</b>, 425–448 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3217002" aria-label="View reference 35 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1214%2F14-BA872" aria-label="View reference 35">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1327.62248" aria-label="View reference 35 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Laplace%20approximation%20for%20logistic%20Gaussian%20process%20density%20estimation%20and%20regression&amp;journal=Bayesian%20Anal.&amp;volume=9&amp;pages=425-448&amp;publication_year=2014&amp;author=Vehtari%2CA&amp;author=Riihim%C3%A4ki%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Watanabe, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Watanabe, S.: Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in " /><p class="c-article-references__text" id="ref-CR36">Watanabe, S.: Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. J. Mach. Learn. Res. <b>11</b>, 3571–3594 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2756194" aria-label="View reference 36 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1242.62024" aria-label="View reference 36 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Asymptotic%20equivalence%20of%20Bayes%20cross%20validation%20and%20widely%20applicable%20information%20criterion%20in%20singular%20learning%20theory&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=11&amp;pages=3571-3594&amp;publication_year=2010&amp;author=Watanabe%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Zhang, MA. Stephens, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Zhang, J., Stephens, M.A.: A new and efficient estimation method for the generalized Pareto distribution. Tech" /><p class="c-article-references__text" id="ref-CR37">Zhang, J., Stephens, M.A.: A new and efficient estimation method for the generalized Pareto distribution. Technometrics <b>51</b>, 316–325 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2751074" aria-label="View reference 37 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1198%2Ftech.2009.08017" aria-label="View reference 37">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20and%20efficient%20estimation%20method%20for%20the%20generalized%20Pareto%20distribution&amp;journal=Technometrics&amp;volume=51&amp;pages=316-325&amp;publication_year=2009&amp;author=Zhang%2CJ&amp;author=Stephens%2CMA">
                        Google Scholar</a></li></ul></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-category="article body" data-track-label="link" href="/article/10.1007/s11222-016-9696-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank Bob Carpenter, Avraham Adler, Joona Karjalainen, Sean Raleigh, Sumio Watanabe, and Ben Lambert for helpful comments, Juho Piironen for R help, Tuomas Sivula for Python port, and the U.S. National Science Foundation, Institute of Education Sciences, and Office of Naval Research for partial support of this research.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><span class="c-article-author-affiliation__address">Department of Computer Science, Helsinki Institute for Information Technology HIIT, Aalto University, Espoo, Finland</span><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Aki Vehtari</li></ul></li><li id="Aff2"><span class="c-article-author-affiliation__address">Department of Statistics, Columbia University, New York, USA</span><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Andrew Gelman</li><li class="c-article-author-affiliation__authors-item"> &amp; Jonah Gabry</li></ul></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-1"><span class="c-article-authors-search__title u-h3 js-search-name">Aki Vehtari</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Aki+Vehtari&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Aki+Vehtari" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Aki+Vehtari%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li><li id="auth-2"><span class="c-article-authors-search__title u-h3 js-search-name">Andrew Gelman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Andrew+Gelman&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Andrew+Gelman" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Andrew+Gelman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li><li id="auth-3"><span class="c-article-authors-search__title u-h3 js-search-name">Jonah Gabry</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jonah+Gabry&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jonah+Gabry" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jonah+Gabry%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p>Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s11222-016-9696-4/email/correspondent/c1/new">Aki Vehtari</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>An erratum to this article is available at <a href="http://dx.doi.org/10.1007/s11222-016-9709-3">http://dx.doi.org/10.1007/s11222-016-9709-3</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix: Implementation in Stan and R</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix: Implementation in Stan and R</h3><h3 class="c-article__sub-heading" id="Sec25">Appendix 1: Stan code for computing and storing the pointwise log-likelihood</h3><p>We illustrate how to write Stan code that computes and stores the pointwise log-likelihood using the arsenic example from Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec22">5.3</a>. We save the program in the file <span class="u-monospace">logistic.stan</span>:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figa_HTML.gif?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figa_HTML.gif" alt="figurea" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div>
                           <p>We have defined the log-likelihood as a vector <span class="u-monospace">log_lik</span> in the generated quantities block so that the individual terms will be saved by Stan.<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> It would seem desirable to compute the terms of the log-likelihood directly without requiring the repetition of code, perhaps by flagging the appropriate lines in the model or by identifying the log likelihood as those lines in the model that are defined relative to the data. But there are so many ways of writing any model in Stan—anything goes as long as it produces the correct log posterior density, up to any arbitrary constant—that we cannot see any general way at this time for computing LOO and WAIC without repeating the likelihood part of the code. The good news is that the additional computations are relatively cheap: sitting as they do in the generated quantities block (rather than in the transformed parameters and model blocks), the expressions for the terms of the log posterior need only be computed once per saved iteration rather than once per HMC leapfrog step, and no gradient calculations are required.</p><h3 class="c-article__sub-heading" id="Sec26">Appendix 2: The <span class="u-monospace">loo</span> R package for LOO and WAIC</h3><p>The <span class="u-monospace">loo</span> R package provides the functions <span class="u-monospace">loo()</span> and <span class="u-monospace">waic()</span> for efficiently computing PSIS-LOO and WAIC for fitted Bayesian models using the methods described in this paper.</p><p>These functions take as their argument an <span class="mathjax-tex">\(S \times n\)</span> log-likelihood matrix, where <i>S</i> is the size of the posterior sample (the number of retained draws) and <i>n</i> is the number of data points.<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> The required means and variances across simulations are calculated and then used to compute the effective number of parameters and LOO or WAIC.</p><p>The <span class="u-monospace">loo()</span> function returns <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo},\widehat{p}_\mathrm{loo},\mathrm{looic} =-2\, \widehat{\mathrm{elpd}}_\mathrm{loo}\)</span> (to provide the output on the conventional scale of “deviance” or AIC),<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup> the pointwise contributions of each of these measures, and standard errors. The <span class="u-monospace">waic()</span> function computes the analogous quantities for WAIC. Also returned by the <span class="u-monospace">loo()</span> function is the estimated shape parameter <span class="mathjax-tex">\({\hat{k}}\)</span> for the generalized Pareto fit to the importance ratios for each leave-one-out distribution. These computations could also be implemented directly in Stan C++, perhaps following the rule that the calculations are performed if there is a variable named <span class="u-monospace">log_lik</span>. The <span class="u-monospace">loo</span> R package, however, is more general and does not require that a model be fit using Stan, as long as an appropriate log-likelihood matrix is supplied.</p><p>
                              <b>Using the </b>
                              <span class="u-monospace">loo</span> 
                              <b>package</b>. Below, we provide R code for preparing and running the logistic regression for the arsenic example in Stan. After fitting the model we then use the <span class="u-monospace">loo</span> package to compute LOO and WAIC.<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup>
                              </p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-b"><figure><div class="c-article-section__figure-content" id="Figb"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figb_HTML.gif?as=webp"></source><img aria-describedby="figure-b-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figb_HTML.gif" alt="figureb" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-b-desc"></div></div></figure></div><p>The printed output shows <span class="mathjax-tex">\(\widehat{\mathrm{elpd}}_\mathrm{loo},\widehat{p}_\mathrm{loo},\mathrm{looic}{} \)</span>, and their standard errors:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-c"><figure><div class="c-article-section__figure-content" id="Figc"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figc_HTML.gif?as=webp"></source><img aria-describedby="figure-c-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figc_HTML.gif" alt="figurec" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-c-desc"></div></div></figure></div>
                           <p>By default, the estimates for the shape parameter <span class="mathjax-tex">\(\hat{k} { ofthe} { generalizedPareto} { distributionare} { alsochecked} { anda} { messageis} { displayedinforming} { theuser} { ifany} \hat{k}{} \)</span> are problematic (see the end of Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec3">2.1</a>). In the example above the message tells us that all of the estimates for <span class="mathjax-tex">\(\hat{k}{} \)</span> are fine. However, if any  <span class="mathjax-tex">\(\hat{k}{} \)</span>  were between  <span class="mathjax-tex">\(1/2\)</span>  and  <span class="mathjax-tex">\(1\)</span>  or greater than  <span class="mathjax-tex">\(1\)</span>  the message would instead look something like this:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-d"><figure><div class="c-article-section__figure-content" id="Figd"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figd_HTML.gif?as=webp"></source><img aria-describedby="figure-d-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figd_HTML.gif" alt="figured" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-d-desc"></div></div></figure></div>
                           <p>If there are any warnings then it can be useful to visualize the estimates to check which data points correspond to the large  <span class="mathjax-tex">\({\hat{k}}{} \)</span>  values. A plot of the  <span class="mathjax-tex">\({\hat{k}}{} \)</span>  estimates can also be generated using <span class="u-monospace">plot(loo1)</span> and the list returned by the <span class="u-monospace">loo</span> function also contains the full vector of  <span class="mathjax-tex">\({\hat{k}}{} \)</span>  values.</p><p>
                              <i>Model comparison</i> To compare this model to a second model on their values of LOO we can use the <span class="u-monospace">compare</span> function:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-e"><figure><div class="c-article-section__figure-content" id="Fige"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fige_HTML.gif?as=webp"></source><img aria-describedby="figure-e-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Fige_HTML.gif" alt="figuree" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-e-desc"></div></div></figure></div>
                           <p>This new object, <span class="u-monospace">loo_diff</span>, contains the estimated difference of expected leave-one-out prediction errors between the two models, along with the standard error:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-f"><figure><div class="c-article-section__figure-content" id="Figf"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figf_HTML.gif?as=webp"></source><img aria-describedby="figure-f-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figf_HTML.gif" alt="figuref" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-f-desc"></div></div></figure></div>
                           <p>
                              <i>Code for WAIC</i> For WAIC the code is analogous and the objects returned have the same structure (except there are no Pareto <span class="mathjax-tex">\(k\)</span> estimates). The <span class="u-monospace">compare()</span> function can also be used to estimate the difference in WAIC between two models:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-g"><figure><div class="c-article-section__figure-content" id="Figg"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figg_HTML.gif?as=webp"></source><img aria-describedby="figure-g-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figg_HTML.gif" alt="figureg" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-g-desc"></div></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec27">Appendix 3: Using the <span class="u-monospace">loo</span> R package with <span class="u-monospace">rstanarm</span> models</h3><p>Here we show how to fit the model for the radon example from Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec17">4.6</a> and carry out PSIS-LOO using the <span class="u-monospace">rstanarm</span> and <span class="u-monospace">loo</span> packages.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-h"><figure><div class="c-article-section__figure-content" id="Figh"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figh_HTML.gif?as=webp"></source><img aria-describedby="figure-h-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figh_HTML.gif" alt="figureh" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-h-desc"></div></div></figure></div>
                           <p>After fitting the models we can pass the fitted model objects <span class="u-monospace">modelA</span> and <span class="u-monospace">modelB</span> directly to <span class="u-monospace">rstanarm</span>’s <span class="u-monospace">loo</span> method and it will call the necessary functions from the <span class="u-monospace">loo</span> package internally.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-i"><figure><div class="c-article-section__figure-content" id="Figi"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figi_HTML.gif?as=webp"></source><img aria-describedby="figure-i-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figi_HTML.gif" alt="figurei" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-i-desc"></div></div></figure></div>
                           <p>This returns:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-j"><figure><div class="c-article-section__figure-content" id="Figj"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figj_HTML.gif?as=webp"></source><img aria-describedby="figure-j-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figj_HTML.gif" alt="figurej" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-j-desc"></div></div></figure></div>
                           <p>If there are warnings about large values of the estimated Pareto shape parameter <span class="mathjax-tex">\({\hat{k}}{} \)</span> for the importance ratios, <span class="u-monospace">rstanarm</span> is also able to automatically carry out the procedure we call PSIS-LOO+ (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s11222-016-9696-4#Sec18">4.7</a>). That is, <span class="u-monospace">rstanarm</span> can refit the model, leaving out these problematic observations one at a time and computing their elpd contributions directly. Then these values are combined with the results from PSIS-LOO for the other observations and returned to the user. We recommended this when there are only a few large  <span class="mathjax-tex">\({{\hat{k}}}{} \)</span>  estimates. If there are many of them then we recommend <span class="mathjax-tex">\(K\)</span>-fold cross-validation, which is also implemented in the latest release of <span class="u-monospace">rstanarm</span>.</p><h3 class="c-article__sub-heading" id="Sec28">Appendix 4: Stan code for <span class="mathjax-tex">\(K\)</span>-fold cross-validation</h3><p>To implement <span class="mathjax-tex">\(K\)</span>-fold cross-validation we repeatedly partition the data, with each partition fitting the model to the training set and using it to predict the holdout set. The code for cross-validation does not look so generic because of the need to repeatedly partition the data. However, in any particular example the calculations are not difficult to implement, the main challenge being the increase in computation time by roughly a factor of <span class="mathjax-tex">\(K\)</span>. We recommend doing the partitioning in R (or Python, or whichever data-processing environment is being used) and then passing the training data and holdout data to Stan in two pieces.</p><p>Again we illustrate with the logistic regression for the arsenic example. We start with the model from above, but we pass in both the training data (<span class="u-monospace">N_t, y_t, X_t</span>) and the holdout set (<span class="u-monospace">N_h, y_h, X_h</span>), augmenting the data block accordingly. We then alter the generated quantities block to operate on the holdout data:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-k"><figure><div class="c-article-section__figure-content" id="Figk"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figk_HTML.gif?as=webp"></source><img aria-describedby="figure-k-desc" src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11222-016-9696-4/MediaObjects/11222_2016_9696_Figk_HTML.gif" alt="figurek" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-k-desc"></div></div></figure></div>
                           <p>LOO could be also implemented in this way, setting  <span class="mathjax-tex">\(N_t\)</span> to <span class="mathjax-tex">\(N-1\)</span>  and  <span class="mathjax-tex">\(N_h\)</span>  to 1. But, as discussed in the article, for large datasets it is more practical to approximate LOO using importance sampling on the draws from the posterior distribution fit to the entire dataset.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-category="article body" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Practical%20Bayesian%20model%20evaluation%20using%20leave-one-out%20cross-validation%20and%20WAIC&amp;author=Aki%20Vehtari%20et%20al&amp;contentID=10.1007%2Fs11222-016-9696-4&amp;publication=0960-3174&amp;publicationDate=2016-08-30&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s11222-016-9696-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s11222-016-9696-4" data-track="click" data-track-action="Click Crossmark" data-track-category="article body" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Vehtari, A., Gelman, A. &amp; Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC.
                    <i>Stat Comput</i> <b>27, </b>1413–1432 (2017). https://doi.org/10.1007/s11222-016-9696-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-category="article body" data-track-label="link" href="/article/10.1007/s11222-016-9696-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2016-01-23">23 January 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2016-08-22">22 August 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2016-08-30">30 August 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2017-09">September 2017</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><a href="https://doi.org/10.1007/s11222-016-9696-4" data-track="click" data-track-action="view doi" data-track-category="article body" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s11222-016-9696-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Bayesian computation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Leave-one-out cross-validation (LOO)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">
                        <i>K</i>-fold cross-validation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Widely applicable information criterion (WAIC)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Stan</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Pareto smoothed importance sampling (PSIS)</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    <div id="pdflink-container" class="download-article test-pdf-link">
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s11222-016-9696-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-category="article body" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    
</div>

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <div class="c-ad c-ad--MPU1">
        <div class="c-ad c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/11222/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=9696;"></div>
        </div>
    </div>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 46.59.59.185</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Stockholm University (2000492649)  - National Library of Sweden BIBSAM - Fully OA (3000092005)  - BIBSAM (3000169192)  - 1018 BIBSAM SWED (3000183675)  - BIBSAM OJA 2011 (3000506941)  - National Library of Sweden (Kungliga Biblioteket) (3002277247)  - SpringerMaterials Tester (3003089239) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 14 14">
            <path d="M13.545 12.648a.641.641 0 01.006.903.646.646 0 01-.903-.006l-2.664-2.663a6.125 6.125 0 11.897-.898l2.664 2.664zm-7.42-1.273a5.25 5.25 0 100-10.5 5.25 5.25 0 000 10.5z"></path>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

